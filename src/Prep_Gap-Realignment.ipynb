{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET=\"testdata\"\n",
    "#TARGET2=\"MCF7\"\n",
    "ver=\"24.4.2.4\"\n",
    "mapping_rate_criteria_trans=0.99\n",
    "buffer=15\n",
    "Nth_hit_buffer=50\n",
    "unpaired_mapping_rate=0.95\n",
    "mapping_gap_rate=0.5\n",
    "mapping_gap_len_criteria=10000\n",
    "clst_percent_cutoff=0.5\n",
    "clst_bp=1000\n",
    "realign_gap_len_criteria=15\n",
    "gap_len_cutoff=15\n",
    "\n",
    "gap_mapping_rate_cutoff=0.5\n",
    "gap_evalue_cutoff=0.05"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 関数のインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from glob import glob \n",
    "#import matplotlib.pyplot as plt\n",
    "#import collections\n",
    "#mport pandas_bj\n",
    "#sns.set_context('notebook')\n",
    "#!jupyter nbextension enable --py --sys-prefix widgetsnbextension\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "# pd.set_option('display.max_columns', 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def my_round_int(x): return int((x * 2 + 1) // 2)\n",
    "\n",
    "def make_name_col_nodev(df):\n",
    "    tmp = df.query('dir1==\"+\" & dir2 == \"+\"')\n",
    "    tmp[\"name2\"] = tmp[\"chr1\"] + \"__\" + tmp[\"end1\"].astype(str) + \"___\" +  tmp[\"chr2\"] + \"__\" + tmp[\"start2\"].astype(str)\n",
    "    tmp2 = df.query('dir1==\"+\"& dir2 == \"-\"')\n",
    "    tmp2[\"name2\"]= tmp2[\"chr1\"] + \"__\" + tmp2[\"end1\"].astype(str) + \"___\" + tmp2[\"chr2\"] + \"__\" + tmp2[\"end2\"].astype(str)\n",
    "    tmp3 = df.query('dir1==\"-\" & dir2 == \"+\"')\n",
    "    tmp3[\"name2\"] = tmp3[\"chr1\"] + \"__\" + tmp3[\"start1\"].astype(str) + \"___\" + tmp3[\"chr2\"] + \"__\" + tmp3[\"start2\"].astype(str)\n",
    "    tmp4 = df.query('dir1==\"-\" & dir2 == \"-\"')\n",
    "    tmp4[\"name2\"]= tmp4[\"chr1\"] + \"__\" + tmp4[\"start1\"].astype(str) + \"___\" + tmp4[\"chr2\"] + \"__\" + tmp4[\"end2\"].astype(str)\n",
    "    df = tmp.append(tmp2)#.sort_values('count',ascending=False)\n",
    "    df = df.append(tmp3)\n",
    "    df = df.append(tmp4)\n",
    "    return df\n",
    "\n",
    "\n",
    "def rm_cross_over(df_fusioncand,buffer=buffer):\n",
    "    df_A = df_fusioncand[df_fusioncand[\"chr1\"] !=df_fusioncand[\"chr2\"]]  # 同じ染色体ではないのでクロスオーバーの対象外\n",
    "    tmp = df_fusioncand[df_fusioncand[\"chr1\"] == df_fusioncand[\"chr2\"]]\n",
    "    drop1 = tmp.query('start1<start2 and end1>end2')\n",
    "    drop2 = tmp.query('start1>start2 and end1<end2')\n",
    "    drop_rid=list(drop1['hit_rid'])+list(drop1['hit_rid'])\n",
    "\n",
    "    df_B_a = tmp[(tmp[\"end1\"] - tmp[\"start2\"] - buffer) *\n",
    "                 (tmp[\"end2\"] - tmp[\"start1\"] - buffer) < 0]\n",
    "    df_B_b = tmp[(tmp[\"end1\"] - tmp[\"start2\"] - buffer) *\n",
    "                 (tmp[\"end2\"] - tmp[\"start1\"] - buffer) < 0]\n",
    "\n",
    "    df_fusioncand_rmcross = pd.concat([df_A, df_B_a, df_B_b])\n",
    "    df_fusioncand_rmcross = df_fusioncand_rmcross.drop_duplicates()\n",
    "    df_fusioncand_rmcross.query('hit_rid not in @drop_rid')\n",
    "    #print('クロスオーバーで除く本数は{}本'.format(df_fusioncand.shape[0] - df_fusioncand_rmcross.shape[0]))\n",
    "    return df_fusioncand_rmcross,drop_rid\n",
    "\n",
    "def judge_cross_over_v3(mmap2_paired,buffer,s,e):\n",
    "    ##print('クエリ側のクロスオーバーの判定')\n",
    "    rids=[]\n",
    "    if mmap2_paired['Nth_hit'].value_counts()[1] != mmap2_paired['Nth_hit'].value_counts()[2]:\n",
    "        drop_rid_nopaired = pd.DataFrame(mmap2_paired.groupby('Qname')['Nth_hit'].nunique() == 1).query('Nth_hit==True').index\n",
    "        mmap2_paired = mmap2_paired.query('Qname not in @drop_rid_nopaired')\n",
    "    for x,rid in enumerate(tqdm(mmap2_paired.Qname.unique())): #read_idごと\n",
    "        mmap2_paired_sub = mmap2_paired.query('Qname==@rid')\n",
    "        N=mmap2_paired_sub.Nth_hit.min()\n",
    "        first_end_pos = int(mmap2_paired_sub.query('Nth_hit==@N')[e].max())\n",
    "        try:\n",
    "            second_start_pos = int(mmap2_paired_sub.query('Nth_hit==@N+1')[s].min())\n",
    "        except:\n",
    "            assert mmap2_paired_sub['Nth_hit'].nunique()==2 ,'Nth_hitが3以上です'\n",
    "            second_start_pos = int(mmap2_paired_sub.query('Nth_hit!=@N')[s].min())\n",
    "        if first_end_pos - buffer > second_start_pos:\n",
    "            rids.append(rid)\n",
    "    return rids\n",
    "    \n",
    "\n",
    "\n",
    "def cul_mapping_rate_all(df):\n",
    "    df_tmp = df.copy()\n",
    "    df_tmp['map_len'] = df_tmp.apply(lambda x: x['Qend']-x['Qstart'], axis=1)\n",
    "    df_tmp=df_tmp.drop_duplicates(['Qname','Nth_hit'])\n",
    "    mapping_rate_all = df_tmp.groupby(['Qname']).agg({'map_len': 'sum', 'Qlen': 'max'}).apply(lambda x: x['map_len']/x['Qlen'], axis=1)\n",
    "    add_df = pd.DataFrame(mapping_rate_all, columns=['mapping_rate_all']).reset_index()\n",
    "    df_out = pd.merge(df, add_df)\n",
    "    return df_out\n",
    "\n",
    "def cul_mapping_GAP(df):\n",
    "    df_tmp = df.copy()\n",
    "    df_tmp['map_len'] = df_tmp.apply(lambda x: x['Qend']-x['Qstart'], axis=1)\n",
    "    df_tmp=df_tmp.drop_duplicates(['Qname','Nth_hit'])\n",
    "    mapping_gap = df_tmp.groupby(['Qname']).agg({'Qend': 'min', 'Qstart': 'max'}).apply(lambda x: x['Qstart']-x['Qend'], axis=1)\n",
    "    add_df = pd.DataFrame(mapping_gap, columns=['mapping_gap']).reset_index()\n",
    "    df_out = pd.merge(df, add_df)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def get_gene_name_start_end(gtf,chr,s,e,buffer=15):\n",
    "    gtf['len']=gtf['end']-gtf['start']\n",
    "    multi_hit_flg=0\n",
    "    gtf_sub1=gtf.query('chr==@chr & start-@buffer <= @s <= end + @buffer and chr==@chr & start-@buffer <= @e <= end+@buffer') #gtfの中にstart,endともに張り付いている\n",
    "    if len(gtf_sub1) >= 1:\n",
    "        gtf_sub1= gtf_sub1.sort_values('len',ascending=False)\n",
    "        g_tmp = gtf_sub1['gene'].unique()\n",
    "        g_multi=\"||\".join(g_tmp)\n",
    "        #g=g_tmp[0]\n",
    "        if len(g_tmp) > 1:\n",
    "            multi_hit_flg=1\n",
    "            g_multi=\"||\".join(g_tmp)\n",
    "    else:\n",
    "        gtf_sub2=gtf.query('chr==@chr & start-@buffer <= @s <= end+@buffer') #gtfの中にstartのみ張り付いている\n",
    "        gtf_sub3=gtf.query('chr==@chr & start-@buffer <= @e <= end+@buffer') #gtfの中にendのみ張り付いている\n",
    "        if len(gtf_sub2)>=1: \n",
    "            gtf_sub2= gtf_sub2.sort_values('len',ascending=False)\n",
    "            g_tmp = gtf_sub2['gene'].unique()\n",
    "            g_multi=\"||\".join(g_tmp)\n",
    "            #g=g_tmp[0]\n",
    "            if len(g_tmp) > 1:\n",
    "                multi_hit_flg=1\n",
    "        elif len(gtf_sub3)>=1: \n",
    "            gtf_sub3= gtf_sub3.sort_values('len',ascending=False)\n",
    "            g_tmp = gtf_sub3['gene'].unique()\n",
    "            g_multi=\"||\".join(g_tmp)\n",
    "            #g=g_tmp[0]\n",
    "            if len(g_tmp)>1:\n",
    "                multi_hit_flg=1\n",
    "        else:\n",
    "            #g='no_hit'\n",
    "            g_multi='intron'\n",
    "        \n",
    "    return pd.Series(g_multi)\n",
    "\n",
    "def prep_gtf(gtf):\n",
    "    gtf['gene_3char'] = gtf['name2'].str[0:3]\n",
    "    gtf = gtf.rename(columns={\"name2\": \"gene\", 'chrom': 'chr','txStart': 'start', 'txEnd': 'end'}).sort_values(['chr', 'start'])\n",
    "    return gtf\n",
    "\n",
    "def add_Nth_hit(mmap2_paired_in,X=Nth_hit_buffer): #30から変更\n",
    "    ##print('Nth hitの判定')\n",
    "    mmap2_paired = mmap2_paired_in.sort_values(['Qname','Qstart','Qend','mapQ','match_rate'],ascending=[True,True,True,False,False])\n",
    "    res=[]\n",
    "    for x,rid in tqdm(enumerate(mmap2_paired.Qname.unique())): #read_idごと\n",
    "        counter=1 #Nth\n",
    "        mmap2_paired_sub = mmap2_paired.query('Qname==@rid')\n",
    "        for n,(_,r) in enumerate(mmap2_paired_sub.iterrows()): #同じread_idでの複数ヒット\n",
    "            if n==0:\n",
    "                res.append(1)\n",
    "            else:\n",
    "                if (mmap2_paired_sub.iloc[n-1][\"Qstart\"] >= mmap2_paired_sub.iloc[n]['Qstart'] - X):\n",
    "                    counter+=0\n",
    "                    res.append(counter)\n",
    "                elif (mmap2_paired_sub.iloc[n-1][\"Qend\"] >= mmap2_paired_sub.iloc[n]['Qend'] -X ): \n",
    "                    counter+=0\n",
    "                    res.append(counter)\n",
    "                else:\n",
    "                    counter+=1\n",
    "                    res.append(counter)\n",
    "    mmap2_paired[\"Nth_hit\"] = res\n",
    "    return mmap2_paired\n",
    "\n",
    "def filter_out_Nth_hit_ov2(df):\n",
    "    cond = df.groupby(\"Qname\").Nth_hit.max() == 2\n",
    "    filterd_qid = cond[cond].index\n",
    "    df_out=df.query('Qname in @filterd_qid')\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def filter_out_mismatch_clst_gname(clst,g1,g2):\n",
    "    clst_1=clst.split(\"--\")[0]\n",
    "    clst_2=clst.split(\"--\")[1]\n",
    "    if (clst_1 in g1 or clst_2 in g1) and (clst_1 in g2 or clst_2 in g2):\n",
    "        ##print(clst,g1,g2,\"_clst_geme_mismatch\")\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def cul_fusioncand_v4_4refseq(mmap2_paired):\n",
    "    ##print('---------------融合点の計算----------------')\n",
    "    col = [\"Qname\",\"Qstart\",\"Qend\",\"dir\", \"Tname\", \"Tstart\", \"Tend\", \"mapQ\",\"match_rate\", \"mapping_rate\",\"gene\"]\n",
    "    out_col = [\"hit_rid\", \"Qstart1\",\"Qend1\",\"dir1\", \"chr1\", \"start1\", \"end1\", \"mapQ1\", \"match_rate1\", \"mapping_rate1\",\"g1\",\"Qstart2\",\"Qend2\",\"dir2\", \"chr2\",\n",
    "               \"start2\", \"end2\", \"mapQ2\", \"match_rate2\", 'mapping_rate2', \"g2\",\"gap_len\",'gap_rate']\n",
    "    arr=[]\n",
    "    for rid in tqdm(mmap2_paired.Qname.unique()):\n",
    "        mmap2_paired_sub = mmap2_paired.query('Qname==@rid')\n",
    "        if mmap2_paired_sub.Nth_hit.nunique()==2:\n",
    "            mmap2_paired_sub_Nth1= mmap2_paired_sub.query('Nth_hit==1')\n",
    "            mmap2_paired_sub_Nth2= mmap2_paired_sub.query('Nth_hit==2')\n",
    "            for i in range(len(mmap2_paired_sub_Nth1)):\n",
    "                for j in range(len(mmap2_paired_sub_Nth2)):\n",
    "                    flont_fp = list(mmap2_paired_sub_Nth1.iloc[i][col]) \n",
    "                    rear_fp = list(mmap2_paired_sub_Nth2.iloc[j][col])[1:]\n",
    "                    gap_len =  mmap2_paired_sub_Nth2.iloc[j]['Qstart']- mmap2_paired_sub_Nth1.iloc[i]['Qend']\n",
    "                    gap_rate =  gap_len / mmap2_paired_sub_Nth1.iloc[i]['Qlen']\n",
    "                    res = flont_fp + rear_fp + [gap_len] + [gap_rate] \n",
    "                    arr.append(res)\n",
    "                    \n",
    "        else:\n",
    "            pass\n",
    "    df_fusioncand=pd.DataFrame(arr)\n",
    "    df_fusioncand.columns=out_col\n",
    "    df_fusioncand = df_fusioncand.drop_duplicates()\n",
    "    return df_fusioncand\n",
    "\n",
    "def cul_fusioncand_v4_quick(mmap2_paired):\n",
    "    ##print('---------------融合点の計算----------------')\n",
    "    col = [\"Qname\",\"Qstart\",\"Qend\",\"dir\", \"Tname\", \"Tstart\", \"Tend\", \"mapQ\",\"match_rate\", \"mapping_rate\"]\n",
    "    out_col = [\"hit_rid\", \"Qstart1\",\"Qend1\",\"dir1\", \"chr1\", \"start1\", \"end1\", \"mapQ1\", \"match_rate1\", \"mapping_rate1\",\"Qstart2\",\"Qend2\",\"dir2\", \"chr2\",\n",
    "               \"start2\", \"end2\", \"mapQ2\", \"match_rate2\", 'mapping_rate2', \"gap_len\",'gap_rate']\n",
    "    arr=[]\n",
    "    for rid in tqdm(mmap2_paired.Qname.unique()):\n",
    "        mmap2_paired_sub = mmap2_paired.query('Qname==@rid')\n",
    "        if mmap2_paired_sub.Nth_hit.nunique()==2:\n",
    "            mmap2_paired_sub_Nth1= mmap2_paired_sub.query('Nth_hit==1')\n",
    "            mmap2_paired_sub_Nth2= mmap2_paired_sub.query('Nth_hit==2')\n",
    "            for i in range(len(mmap2_paired_sub_Nth1)):\n",
    "                for j in range(len(mmap2_paired_sub_Nth2)):\n",
    "                    flont_fp = list(mmap2_paired_sub_Nth1.iloc[i][col]) \n",
    "                    rear_fp = list(mmap2_paired_sub_Nth2.iloc[j][col])[1:]\n",
    "                    gap_len =  mmap2_paired_sub_Nth2.iloc[j]['Qstart']- mmap2_paired_sub_Nth1.iloc[i]['Qend']\n",
    "                    gap_rate =  gap_len / mmap2_paired_sub_Nth1.iloc[i]['Qlen']\n",
    "                    res = flont_fp + rear_fp + [gap_len] + [gap_rate] \n",
    "                    arr.append(res)\n",
    "                    \n",
    "        else:\n",
    "            pass\n",
    "            ### 3箇所以上にマッピングされるものは候補から除外\n",
    "        df_fusioncand=pd.DataFrame(arr)\n",
    "        df_fusioncand.columns=out_col\n",
    "        df_fusioncand = df_fusioncand.drop_duplicates()\n",
    "    return df_fusioncand\n",
    "\n",
    "\n",
    "def prep_paf_edge_4gaponly(path):\n",
    "    paf_col=[\"Qname\", \"Qlen\", \"Qstart\", \"Qend\", \"dir\", \"Tname\", \"Tlen\", \"Tstart\", \"Tend\", \"match\", \"block\", \"mapQ\"]\n",
    "    df = pd.read_table(path, index_col=False, header=None, names=paf_col, usecols=range(0, 12)).drop('Tlen',axis=1)\n",
    "    if len(df)>0:\n",
    "        #df[\"Nth_hit\"]=df['Qname'].str.rsplit(\"_\",2).str[1].str[0].astype(int)\n",
    "        df[\"pat\"]=df['Qname'].str.rsplit(\"_\",1).str[-1].str[0]\n",
    "        df['Qname'] = df['Qname'].str.split(',').str[0].str.rsplit(\"_\",4).str[0]\n",
    "        #df['gene'] = df.apply(lambda x: get_gene_name_from_rid(x['Qname']), axis=1)\n",
    "        df['match_rate'] = (df['match'])/(df['Qend']-df['Qstart']+1)\n",
    "        df['mapping_rate'] = (df['Qend']-df['Qstart']+1)/df['Qlen']\n",
    "        df.sort_values(['Qname','Qstart'],ascending=[True,True],inplace=True)\n",
    "        return df\n",
    "\n",
    "def prep_subseq_read_gaponly(df_in):\n",
    "    res_df=pd.DataFrame()\n",
    "    ##print('----start prep_subseq_read gap only -------')\n",
    "    df = df_in[['Qname','Qlen' ,'Qstart', 'Qend','Nth_hit']].drop_duplicates()\n",
    "    out_col=['Qname', 'Qstart', 'Qend']\n",
    "    for rid in tqdm(df.Qname.unique()):\n",
    "        df_sub = df.query('Qname in @rid')\n",
    "        Nths = df_sub.Nth_hit.nunique()\n",
    "        if Nths==2: \n",
    "            df_sub1=df_sub.query('Nth_hit == 1')\n",
    "            df_sub2=df_sub.query('Nth_hit == 2')\n",
    "            N=0\n",
    "            for (_,r1) in df_sub1.iterrows():\n",
    "                for (_,r2) in df_sub2.iterrows():\n",
    "                    res = pd.Series([rid, int(r1['Qend']),int(r2['Qstart'])],index=out_col)\n",
    "                    res_df = res_df.append(res,ignore_index=True)\n",
    "                    N=N+1\n",
    "\n",
    "    res_df = res_df.astype({'Qend':int,'Qstart':int}).sort_values(['Qname','Qstart','Qend'])[out_col].drop_duplicates(subset=['Qname','Qstart','Qend'])\n",
    "    assert res_df.Qname.nunique()==df.Qname.nunique()\n",
    "    return res_df\n",
    "\n",
    "def filter_only_2pairofgene(df_fusioncand_in,g1_col,g2_col):\n",
    "    df_fusioncand = df_fusioncand_in.copy()\n",
    "    cond1=df_fusioncand.groupby('hit_rid')[g1_col].nunique()==1\n",
    "    g1_uniq_rid=cond1[cond1].index\n",
    "    cond2=df_fusioncand.groupby('hit_rid')[g2_col].nunique()==1\n",
    "    g2_uniq_rid=cond2[cond2].index\n",
    "    g1_g2_uniq_rid=set(g1_uniq_rid)&set(g2_uniq_rid)\n",
    "    df_fusioncand = df_fusioncand.query('hit_rid in @g1_g2_uniq_rid')\n",
    "    return df_fusioncand\n",
    "\n",
    "\n",
    "def reclst_stop(df_in,dont_reclust_cutoff):\n",
    "    df_fusioncand=df_in.copy()\n",
    "    df_fusioncand.loc[(df_fusioncand['clst_count_1st']>=dont_reclust_cutoff) & (df_fusioncand[\"clst_final\"]!=df_fusioncand['clst_1st']),'g1_clst']=df_fusioncand['g1_clst_1st']\n",
    "    df_fusioncand.loc[(df_fusioncand['clst_count_1st']>=dont_reclust_cutoff) & (df_fusioncand[\"clst_final\"]!=df_fusioncand['clst_1st']),'g2_clst']=df_fusioncand['g2_clst_1st']\n",
    "    df_fusioncand.loc[(df_fusioncand['clst_count_1st']>=dont_reclust_cutoff) & (df_fusioncand[\"clst_final\"]!=df_fusioncand['clst_1st']),'clst_final']=df_fusioncand['clst_1st']\n",
    "    df_fusioncand.loc[(df_fusioncand['clst_count_1st']>=dont_reclust_cutoff) & (df_fusioncand[\"clst_final\"]!=df_fusioncand['clst_1st']),'clst_count']=df_fusioncand['clst_count_1st']\n",
    "    df_fusioncand.loc[(df_fusioncand['clst_count_1st']>=dont_reclust_cutoff) & (df_fusioncand[\"clst_final\"]!=df_fusioncand['clst_1st']),'support_read']=df_fusioncand['support_read_1st']\n",
    "    return df_fusioncand\n",
    "\n",
    "\n",
    "def drop_different_genepair_reseq(g1,g2,g1_r,g2_r):\n",
    "    if (g1 in g1_r or g1 in g2_r) and (g2 in g1_r or g2 in g2_r):\n",
    "        res=0\n",
    "    elif (g1_r in g1 or g1_r in g2) and (g2_r in g1 or g2_r in g2):\n",
    "        res=0\n",
    "    else:\n",
    "        res=1\n",
    "    return res\n",
    "\n",
    "\n",
    "def add_g1_g2_clst_col_v3(df_fusioncand_clst_tmp,clst_col,g1_col,g2_col,out_g1_col,out_g2_col,out_g1_pct_col,out_g2_pct_col,criteria):\n",
    "   res_df=pd.DataFrame()\n",
    "   for clst in df_fusioncand_clst_tmp[clst_col].unique():\n",
    "      df_fusioncand_clust_sub= df_fusioncand_clst_tmp[df_fusioncand_clst_tmp[clst_col]==clst]\n",
    "      ratio1 = df_fusioncand_clust_sub[g1_col].value_counts(normalize=True)\n",
    "      ratio2 = df_fusioncand_clust_sub[g2_col].value_counts(normalize=True)\n",
    "      cond1 = ratio1 >= criteria\n",
    "      cond2 = ratio2 >= criteria\n",
    "      g1_multi_clst=\"_\".join(list((cond1.index[cond1])))\n",
    "      g2_multi_clst=\"_\".join(list((cond2.index[cond2])))\n",
    "      g1_multi_clst_pct=\"_\".join(list((ratio1[cond1].round(2)*100).astype(int).astype(str) + '%'))\n",
    "      g2_multi_clst_pct=\"_\".join(list((ratio2[cond2].round(2)*100).astype(int).astype(str) + '%'))\n",
    "      #g1_multi_clst_pct=ratio1[cond1][0].round(2)\n",
    "      #g2_multi_clst_pct=ratio2[cond2][0].round(2)\n",
    "      \n",
    "      df_fusioncand_clust_sub[out_g1_col]=g1_multi_clst\n",
    "      df_fusioncand_clust_sub[out_g2_col]=g2_multi_clst\n",
    "      df_fusioncand_clust_sub[out_g1_pct_col]=g1_multi_clst_pct\n",
    "      df_fusioncand_clust_sub[out_g2_pct_col]=g2_multi_clst_pct\n",
    "      res_df = res_df.append(df_fusioncand_clust_sub)\n",
    "   return res_df\n",
    "\n",
    "def filter_out_Nth_hit(df,N):\n",
    "    cond = df.groupby(\"Qname\").Nth_hit.max() == N\n",
    "    filterd_qid = cond[cond].index\n",
    "    df_out=df.query('Qname not in @filterd_qid')\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def filter_refseq_paf(local_file_path_refseq,gtf_path,N=1):\n",
    "    gtf = pd.read_csv(gtf_path, usecols=[0,5],names=['Tname','gene'],header=0,index_col=False)\n",
    "    paf_col = [\"Qname\", \"Qlen\", \"Qstart\", \"Qend\", \"dir\", \"Tname\",\"Tlen\", \"Tstart\", \"Tend\", \"match\", \"block\", \"mapQ\"]\n",
    "    mmap2_refseq = pd.read_table(local_file_path_refseq, index_col=False, header=None,names=paf_col, usecols=range(0, 12)).drop('Tlen', axis=1)\n",
    "    mmap2_refseq['Tname'] = mmap2_refseq['Tname'].str.split(\"_\").str[2]\n",
    "    mmap2_refseq = pd.merge(mmap2_refseq, gtf,how='left')\n",
    "    \n",
    "    #1geneにしか当たらないリードをdrop\n",
    "    cond1=mmap2_refseq.groupby('Qname')['gene'].nunique()!=N\n",
    "    filterd_qid1=cond1[cond1].index\n",
    "    mmap2_refseq_cand1=mmap2_refseq.query('Qname in @filterd_qid1')\n",
    "    \n",
    "    #startとendが一致しているものは落とす\n",
    "    cond2 = (mmap2_refseq_cand1.groupby('Qname')['Qstart'].nunique() != 1) & (mmap2_refseq_cand1.groupby('Qname')['Qend'].nunique() != 1)\n",
    "    filterd_qid2=cond2[cond2].index\n",
    "    mmap2_refseq_cand2=mmap2_refseq_cand1.query('Qname in @filterd_qid2')\n",
    "    mmap2_refseq_cand2['match_rate'] = (mmap2_refseq_cand2['match']) / (mmap2_refseq_cand2['Qend']-mmap2_refseq_cand2['Qstart']+1)\n",
    "    mmap2_refseq_cand2['mapping_rate'] = (mmap2_refseq_cand2['Qend']-mmap2_refseq_cand2['Qstart']+1)/mmap2_refseq_cand2['Qlen']\n",
    "    cond2 = (mmap2_refseq_cand2.groupby('Qname')['Qstart'].nunique() != 1) & (mmap2_refseq_cand2.groupby('Qname')['Qend'].nunique() != 1)\n",
    "    filterd_qid2=cond2[cond2].index\n",
    "    mmap2_refseq_cand2=mmap2_refseq_cand2.query('Qname in @filterd_qid2').sort_values('Qname')\n",
    "    multi_gene_id=mmap2_refseq_cand2.Qname.unique()\n",
    "    \n",
    "    return multi_gene_id,mmap2_refseq, mmap2_refseq_cand2\n",
    "\n",
    "def prep_paf_file_v3(paffile_path, filterd_qid, Nth_hit_flg=1):\n",
    "    paf_col = [\"Qname\", \"Qlen\", \"Qstart\", \"Qend\", \"dir\", \"Tname\",\"Tlen\", \"Tstart\", \"Tend\", \"match\", \"block\", \"mapQ\"]\n",
    "    mmap2 = pd.read_table(paffile_path, index_col=False, header=None,names=paf_col, usecols=range(0, 12)).drop('Tlen', axis=1)\n",
    "    if len(filterd_qid)!=0: \n",
    "        mmap2_cand1=mmap2.query('Qname in @filterd_qid')\n",
    "    else:\n",
    "        mmap2_cand1=mmap2.copy()\n",
    "    cond3 = (mmap2_cand1.groupby('Qname')['Qstart'].nunique() != 1) & (mmap2_cand1.groupby('Qname')['Qend'].nunique() != 1)\n",
    "    filterd_qid3 = cond3[cond3].index\n",
    "    mmap2_cand2 = mmap2_cand1.query('Qname in @filterd_qid3')\n",
    "    mmap2_cand2['Qhit']=mmap2_cand2['Qend']-mmap2_cand2['Qstart']+1\n",
    "    mmap2_cand2['match_rate'] = (mmap2_cand2['match']) / (mmap2_cand2['Qend']-mmap2_cand2['Qstart']+1)\n",
    "    mmap2_cand2['mapping_rate'] = (mmap2_cand2['Qend']-mmap2_cand2['Qstart']+1)/mmap2_cand2['Qlen']\n",
    "    if Nth_hit_flg==1:\n",
    "        mmap2['Tname'] = mmap2['Tname'].str.split(\"_\").str[2]\n",
    "        mmap2_cand2 = add_Nth_hit(mmap2_cand2)\n",
    "    return mmap2,mmap2_cand2\n",
    "\n",
    "def flging_gap_use_read(df):\n",
    "    Qnames=df.query('gene==gene_gap').Qname.unique()\n",
    "    df.loc[df['Qname'].isin(Qnames),'gap_use']=1\n",
    "    return df\n",
    "        \n",
    "def filterout_multigene_hit(df,col):\n",
    "    cond=df.groupby('Qname')[col].nunique()==1\n",
    "    uniq_hit_rid=cond[cond].index\n",
    "    df_filtered=df.query('Qname in @uniq_hit_rid')\n",
    "    return df_filtered\n",
    "\n",
    "def cul_fusioncand_v4(mmap2_paired):\n",
    "    ##print('---------------融合点の計算----------------')\n",
    "    col = [\"Qname\",\"Qstart\",\"Qend\",\"dir\", \"Tname\", \"Tstart\",\"Tend\",\"mapQ\",\"gene\"]\n",
    "    #out_col = [\"hit_rid\", \"Qstart1\",\"Qend1\",\"dir1\", \"chr1\", \"start1\", \"end1\", \"mapQ1\", \"match_rate1\", \"mapping_rate1\",\"Qstart2\",\"Qend2\",\"dir2\", \"chr2\",\n",
    "    #          \"start2\", \"end2\", \"mapQ2\", \"match_rate2\", 'mapping_rate2', \"gap_len\",'gap_rate']\n",
    "    out_col = [\"hit_rid\", \"Qstart1\",\"Qend1\",\"dir1\", \"chr1\", \"start1\", \"end1\", \"mapQ1\",\"gene1\",\"Qstart2\",\"Qend2\",\"dir2\", \"chr2\",\n",
    "               \"start2\", \"end2\", \"mapQ2\", \"gene2\",\"gap_len\",'gap_rate']\n",
    "    arr=[]\n",
    "    for rid in tqdm(mmap2_paired.Qname.unique()):\n",
    "        mmap2_paired_sub = mmap2_paired.query('Qname==@rid')\n",
    "        if mmap2_paired_sub.Nth_hit.nunique()==2:\n",
    "            mmap2_paired_sub_Nth1= mmap2_paired_sub.query('Nth_hit==1')\n",
    "            mmap2_paired_sub_Nth2= mmap2_paired_sub.query('Nth_hit==2')\n",
    "            for i in range(len(mmap2_paired_sub_Nth1)):\n",
    "                for j in range(len(mmap2_paired_sub_Nth2)):\n",
    "                    flont_fp = list(mmap2_paired_sub_Nth1.iloc[i][col]) \n",
    "                    rear_fp = list(mmap2_paired_sub_Nth2.iloc[j][col])[1:]\n",
    "                    gap_len =  mmap2_paired_sub_Nth2.iloc[j]['Qstart']- mmap2_paired_sub_Nth1.iloc[i]['Qend']\n",
    "                    gap_rate =  gap_len / mmap2_paired_sub_Nth1.iloc[i]['Qlen']\n",
    "                    res = flont_fp + rear_fp + [gap_len] + [gap_rate] \n",
    "                    arr.append(res)\n",
    "                    \n",
    "        else:\n",
    "            pass\n",
    "            ### 3箇所以上にマッピングされるものは候補から除外\n",
    "        df_fusioncand=pd.DataFrame(arr)\n",
    "        df_fusioncand.columns=out_col\n",
    "        df_fusioncand = df_fusioncand.drop_duplicates()\n",
    "    return df_fusioncand\n",
    "\n",
    "def change_order_g1g2_v3(df_fusioncand_in,type):\n",
    "    df_fusioncand = df_fusioncand_in.copy()\n",
    "    if type==\"refseq\":\n",
    "        g1_col = ['dir1', 'chr1', 'start1', 'end1','mapQ1','g1']\n",
    "        g2_col = ['dir2', 'chr2', 'start2', 'end2','mapQ2','g2']\n",
    "        condition=(df_fusioncand['g1'] > df_fusioncand['g2']) | ((df_fusioncand['g1'] == df_fusioncand['g2']) & (df_fusioncand['start1'] > df_fusioncand['start2']))\n",
    "    else:\n",
    "        g1_col = ['dir1', 'chr1', 'start1', 'end1','mapQ1' ]\n",
    "        g2_col = ['dir2', 'chr2', 'start2', 'end2','mapQ2']\n",
    "        condition=(df_fusioncand['chr1'] > df_fusioncand['chr2']) | ((df_fusioncand['chr1'] == df_fusioncand['chr2']) & (df_fusioncand['start1'] > df_fusioncand['start2']))\n",
    "    df_cand = df_fusioncand[condition]\n",
    "    new2_df = df_cand[g1_col]\n",
    "    new1_df = df_cand[g2_col]\n",
    "    new1_df.columns=g1_col\n",
    "    new2_df.columns=g2_col\n",
    "    df_fusioncand.loc[condition, g1_col +g2_col] = pd.concat([new1_df, new2_df], axis=1)\n",
    "\n",
    "    return df_fusioncand\n",
    "\n",
    "def get_major_clst(clst1,count1,clst2,count2):\n",
    "    if count1 >= count2:\n",
    "        return clst1\n",
    "    else:\n",
    "        return clst2\n",
    "\n",
    "def make_descendants_table(df,df_count):\n",
    "    import networkx as nx\n",
    "    x=df\n",
    "    G = nx.DiGraph()\n",
    "    G.add_weighted_edges_from([tuple(x) for x in x.values])\n",
    "    nx.info(G)\n",
    "    df_descendants=pd.DataFrame(columns=['clst_1st','clst_final'])\n",
    "    for node in G.nodes():\n",
    "        descendant=nx.descendants(G, node)\n",
    "        if len(descendant)==0:\n",
    "            descendant_=node\n",
    "        else:\n",
    "            descendant_=(list(descendant)[0])\n",
    "        var_ser=pd.Series([node,descendant_],index=df_descendants.columns)\n",
    "        df_descendants=df_descendants.append( var_ser, ignore_index=True )\n",
    "    df_descendants=df_descendants.query('clst_1st!=clst_final')\n",
    "    df_descendants=pd.merge(df_descendants,df_count.drop_duplicates(),on='clst_1st',how='left')\n",
    "    return df_descendants\n",
    "\n",
    "def re_clst_miner2major_v3(df_in,clst_col,clst_count_col,g1_r,g2_r,dont_reclust_cutoff):\n",
    "    df=df_in.copy()\n",
    "    df_clst_count=df[[clst_col,clst_count_col,g1_r,g2_r]].drop_duplicates()\n",
    "    df_4reclst = df[[clst_col,clst_count_col,g1_r,g2_r]]\n",
    "    df_case1=pd.merge(df_4reclst,df_clst_count,on=g1_r,suffixes=['', '_upd1']).query('clst_count<clst_count_upd1')[[clst_col,'clst_1st_upd1',clst_count_col,'clst_count_upd1']].drop_duplicates()\n",
    "    df_case1.columns=df_case1.columns.str.replace('upd1','upd')\n",
    "    df_case2=pd.merge(df_4reclst,df_clst_count,on=g2_r,suffixes=['', '_upd2']).query('clst_count<clst_count_upd2')[[clst_col,'clst_1st_upd2',clst_count_col,'clst_count_upd2']].drop_duplicates()\n",
    "    df_case2.columns=df_case2.columns.str.replace('upd2','upd')\n",
    "\n",
    "    df_clst_f2=pd.concat([df_case1,df_case2]).sort_values('clst_count_upd',ascending=False)\\\n",
    "        .drop_duplicates(clst_col)\\\n",
    "        .query('clst_count<=@dont_reclust_cutoff')\\\n",
    "        .drop(\"clst_count_upd\",axis=1).rename(columns={'clst_1st_upd':'clst_final'})\n",
    "        \n",
    "    \n",
    "    df_clst_count_in=df_clst_count[[clst_col,clst_count_col]].drop_duplicates()\n",
    "    df_clst_f2_upd = make_descendants_table(df_clst_f2,df_clst_count_in) ##ネットワーク使ってアップデート\n",
    "    df_clst_f2_upd2 = make_descendants_table(df_clst_f2_upd,df_clst_count_in)\n",
    "    df_clst_f2_upd3 = make_descendants_table(df_clst_f2_upd2,df_clst_count_in)\n",
    "    assert df_clst_f2_upd3.equals(df_clst_f2_upd2)\n",
    "     \n",
    "    df_out = pd.merge(df,df_clst_f2_upd3,on=[clst_col,clst_count_col],how='left')\n",
    "    df_out.loc[df_out.clst_final.isna(),'clst_final']=df_out[clst_col]\n",
    "    return df_out\n",
    "\n",
    "def get_cytoband_start_end(cytoband,chr,s,e,buffer=15):\n",
    "    cytoband['len']=cytoband['end']-cytoband['start']\n",
    "    cytoband_col=\"chr_pq\"\n",
    "    multi_hit_flg=0\n",
    "    gtf_sub1=cytoband.query('chr==@chr & start-@buffer <= @s <= end + @buffer and chr==@chr & start-@buffer <= @e <= end+@buffer') #gtfの中にstart,endともに張り付いている\n",
    "    if len(gtf_sub1) >= 1:\n",
    "        gtf_sub1= gtf_sub1.sort_values('len',ascending=False)\n",
    "        g_tmp = gtf_sub1[cytoband_col].unique()\n",
    "        g_multi=\"||\".join(g_tmp)\n",
    "        #g=g_tmp[0]\n",
    "        if len(g_tmp) > 1:\n",
    "            multi_hit_flg=1\n",
    "            g_multi=\"||\".join(g_tmp)\n",
    "    else:\n",
    "        cytoband2=cytoband.query('chr==@chr & start-@buffer <= @s <= end+@buffer') #gtfの中にstartのみ張り付いている\n",
    "        cytoband3=cytoband.query('chr==@chr & start-@buffer <= @e <= end+@buffer') #gtfの中にendのみ張り付いている\n",
    "        if len(cytoband2)>=1: \n",
    "            cytoband2= cytoband2.sort_values('len',ascending=False)\n",
    "            g_tmp = cytoband2[cytoband_col].unique()\n",
    "            g_multi=\"||\".join(g_tmp)\n",
    "            #g=g_tmp[0]\n",
    "            if len(g_tmp) > 1:\n",
    "                multi_hit_flg=1\n",
    "        elif len(cytoband3)>=1: \n",
    "            cytoband3= cytoband3.sort_values('len',ascending=False)\n",
    "            g_tmp = cytoband3[cytoband_col].unique()\n",
    "            g_multi=\"||\".join(g_tmp)\n",
    "            #g=g_tmp[0]\n",
    "            if len(g_tmp)>1:\n",
    "                multi_hit_flg=1\n",
    "        \n",
    "    return pd.Series(g_multi)\n",
    "\n",
    "def add_TF_columns_genename_v2(true_gA,true_gB, g1, g2):\n",
    "    res=0;index=9999\n",
    "    # if pd.isna(g1) or pd.isna(g2):\n",
    "    #     return pd.Series([res,index])\n",
    "    for row_g1,true_gA_sub in enumerate(true_gA):\n",
    "        if true_gA_sub in g1 or true_gA_sub in g2:\n",
    "            for row_g2,true_gB_sub in enumerate(true_gB):\n",
    "                if (true_gB_sub in g1 or true_gB_sub in g2) and (row_g1==row_g2):\n",
    "                    res=1\n",
    "                    index=row_g1\n",
    "    return pd.Series([res,index])\n",
    "\n",
    "def add_clst_count_v2(df,clst_col,col_name,support_flg=1):\n",
    "    df_clst_count = df[clst_col].value_counts().to_frame().reset_index().rename(columns={clst_col: \"clst_count\", 'index': clst_col})\n",
    "    df_out=pd.merge(df,df_clst_count,on=clst_col)\n",
    "    if support_flg==1:\n",
    "        df_out[\"support_read\"]=df_out.apply(lambda x:int(x[\"clst_count\"])*float(x['g1_clst_pct'].split(\"_\")[0].replace(\"%\",\"\"))*float(x['g2_clst_pct'].split(\"_\")[0].replace(\"%\",\"\"))*0.01*0.01,axis=1)\n",
    "    else:\n",
    "        pass\n",
    "    assert df[clst_col].nunique()==df_out[clst_col].nunique()\n",
    "    return df_out\n",
    "\n",
    "def make_breakpoint_divX(X,by=10000):\n",
    "    bp=X\n",
    "    chr1=bp.split(\"___\")[0].split(\"__\")[0]\n",
    "    pos1=bp.split(\"___\")[0].split(\"__\")[1]\n",
    "    chr2=bp.split(\"___\")[1].split(\"__\")[0]\n",
    "    pos2=bp.split(\"___\")[1].split(\"__\")[1]\n",
    "    pos1_div=str(my_round_int(int(pos1) / by))\n",
    "    pos2_div=str(my_round_int(int(pos2) / by))\n",
    "    res=chr1+ \"__\" + pos1_div + \"___\" +chr2+ \"__\" + pos2_div\n",
    "    return res\n",
    "\n",
    "def prep_blat_edge_4gaponly(path,topscore=1):\n",
    "    df=pd.read_table(path,names=[\"Qname\",\"Tname\",\"identity\",\"alignment_length\",\"mismatches\",\"gap_openings\",\"Qstart\",\"Qend\",\"Tstart\",\"Tend\",\"evalue\",\"bitscore\"])\n",
    "    if topscore==1:\n",
    "        df=df.sort_values(['Qname','evalue','bitscore'],ascending=[True,True,False])\n",
    "        df=(df.groupby(['Qname'],as_index=False).apply(select,col='evalue',kind='min'))\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "    df.sort_values(['Qname','Qstart'],ascending=[True,True],inplace=True)\n",
    "    #df=df.query('evalue<=@cutoff')\n",
    "    return df\n",
    "\n",
    "def select(df, **kwargs):  # colに列名,kindに最大もしくは最小\n",
    "\n",
    "    if kwargs['kind'] == 'min':\n",
    "        val_r = df[df[kwargs['col']] == min(df[kwargs['col']])]\n",
    "    elif kwargs['kind'] == 'max':\n",
    "        val_r = df[df[kwargs['col']] == max(df[kwargs['col']])]\n",
    "    else:\n",
    "        raise Exception(\"パラメータ不正\")\n",
    "\n",
    "    # 全く同じ行があった場合は削除\n",
    "    val_r = val_r.drop_duplicates()\n",
    "\n",
    "    return val_r\n",
    "\n",
    "def add_exon_s_e(df,gtf_exon_path):\n",
    "    df=df.assign(gene2=df['gene'].str.split(\"\\|\\|\")).explode('gene2')\n",
    "    df_exon=pd.read_csv(gtf_exon_path)\n",
    "    df_merged=pd.merge(df,df_exon[['gene','exstart','exend']],left_on='gene2',right_on=\"gene\")\n",
    "    df_merged['diff_s']=abs(df_merged['Tstart']-df_merged['exstart'])\n",
    "    df_merged['diff_e']=abs(df_merged['exend']-df_merged['Tend'])\n",
    "\n",
    "    tmp1=df_merged.groupby(['Qname','Nth_hit'],as_index=False).diff_s.min()\n",
    "    tmp2=df_merged.groupby(['Qname','Nth_hit'],as_index=False).diff_e.min()\n",
    "    df_merged_t=pd.merge(df_merged,tmp1,how='inner')\n",
    "    df_start=df_merged_t[['Qname','Nth_hit','exstart','diff_s']].drop_duplicates()\n",
    "    df_merged_t=pd.merge(df_merged,tmp2,how='inner')\n",
    "    df_end=df_merged_t[['Qname','Nth_hit','exend','diff_e']].drop_duplicates()\n",
    "    df_s_e=pd.merge(df_start,df_end)\n",
    "    df_out=pd.merge(df,df_s_e).sort_values(['Qname','Nth_hit'])\n",
    "    df_out['diff_s']=df_out['Tstart']-df_out['exstart']\n",
    "    df_out['diff_e']=df_out['exend']-df_out['Tend']\n",
    "    return df_out\n",
    "\n",
    "def fix_start_end(df_in):\n",
    "    df=df_in.copy()\n",
    "    df[['Qstart_fix','Qend_fix','Tstart_fix','Tend_fix']]=df[['Qstart','Qend','Tstart','Tend']].copy()\n",
    "    #Nth1 dir +\n",
    "    df.loc[(df['Nth_hit']==1) & (df['dir']==\"+\"),\"Tend_fix\"]=df['exend']\n",
    "    df.loc[(df['Nth_hit']==1) & (df['dir']==\"+\"),'Qend_fix']=df['Qend']+df['diff_e']\n",
    "    #Nth1 dir -\n",
    "    df.loc[(df['Nth_hit']==1) & (df['dir']==\"-\"),\"Tstart_fix\"]=df['exstart']\n",
    "    df.loc[(df['Nth_hit']==1) & (df['dir']==\"-\"),'Qend_fix']=df['Qend']+df['diff_s']\n",
    "    #df.loc[(df['Nth_hit']==1) & (df['dir']==\"-\")]\n",
    "\n",
    "    #Nth2 dir +\n",
    "    df.loc[(df['Nth_hit']==2) & (df['dir']==\"+\"),\"Tstart_fix\"]=df['exstart']\n",
    "    df.loc[(df['Nth_hit']==2) & (df['dir']==\"+\"),'Qstart_fix']=df['Qstart']-df['diff_s']\n",
    "    #Nth1 dir -\n",
    "    df.loc[(df['Nth_hit']==2) & (df['dir']==\"-\"),\"Tend_fix\"]=df['exend']\n",
    "    df.loc[(df['Nth_hit']==2) & (df['dir']==\"-\"),'Qstart_fix']=df['Qstart']-df['diff_e']\n",
    "    return df\n",
    "            \n",
    "def prep_subseq_read_gaponly_v2(df_in):\n",
    "    res_df=pd.DataFrame()\n",
    "    ##print('----start prep_subseq_read gap only -------')\n",
    "    df = df_in[['Qname','Qlen' ,'Qstart_fix', 'Qend_fix','Nth_hit']].drop_duplicates()\n",
    "    out_col=['Qname', 'Qstart', 'Qend']\n",
    "    for rid in tqdm(df.Qname.unique()):\n",
    "        df_sub = df.query('Qname in @rid')\n",
    "        Nths = df_sub.Nth_hit.nunique()\n",
    "        if Nths==2: \n",
    "            df_sub1=df_sub.query('Nth_hit == 1')\n",
    "            df_sub2=df_sub.query('Nth_hit == 2')\n",
    "            N=0\n",
    "            for (_,r1) in df_sub1.iterrows():\n",
    "                for (_,r2) in df_sub2.iterrows():\n",
    "                    res = pd.Series([rid, int(r1[\"Qend_fix\"]),int(r2[\"Qstart_fix\"])],index=out_col)\n",
    "                    res=pd.DataFrame(res).T\n",
    "                    res_df = pd.concat([res_df, res], ignore_index=True, axis=0)\n",
    "                    \n",
    "                    N=N+1\n",
    "    return res_df\n",
    "\n",
    "\n",
    "def judge_cross_over_v3(mmap2_paired,buffer,s,e):\n",
    "    ##print('クエリ側のクロスオーバーの判定')\n",
    "    rids=[]\n",
    "    if mmap2_paired['Nth_hit'].value_counts()[1] != mmap2_paired['Nth_hit'].value_counts()[2]:\n",
    "        drop_rid_nopaired = pd.DataFrame(mmap2_paired.groupby('Qname')['Nth_hit'].nunique() == 1).query('Nth_hit==True').index\n",
    "        mmap2_paired = mmap2_paired.query('Qname not in @drop_rid_nopaired')\n",
    "    for x,rid in enumerate(tqdm(mmap2_paired.Qname.unique())): #read_idごと\n",
    "        mmap2_paired_sub = mmap2_paired.query('Qname==@rid')\n",
    "        N=mmap2_paired_sub.Nth_hit.min()\n",
    "        first_end_pos = int(mmap2_paired_sub.query('Nth_hit==@N')[e].max())\n",
    "        try:\n",
    "            second_start_pos = int(mmap2_paired_sub.query('Nth_hit==@N+1')[s].min())\n",
    "        except:\n",
    "            assert mmap2_paired_sub['Nth_hit'].nunique()==2 ,'Nth_hitが3以上です'\n",
    "            second_start_pos = int(mmap2_paired_sub.query('Nth_hit!=@N')[s].min())\n",
    "        if first_end_pos - buffer > second_start_pos:\n",
    "            rids.append(rid)\n",
    "    return rids\n",
    "\n",
    "def add_exon_s_e(df,gtf_exon_path):\n",
    "    df=df.assign(gene2=df['gene'].str.split(\"\\|\\|\")).explode('gene2')\n",
    "    df_exon=pd.read_csv(gtf_exon_path)\n",
    "    df_merged=pd.merge(df,df_exon[['gene','exstart','exend']],left_on='gene2',right_on=\"gene\")\n",
    "    df_merged['diff_s']=abs(df_merged['Tstart']-df_merged['exstart'])\n",
    "    df_merged['diff_e']=abs(df_merged['exend']-df_merged['Tend'])\n",
    "\n",
    "    tmp1=df_merged.groupby(['Qname','Nth_hit'],as_index=False).diff_s.min()\n",
    "    tmp2=df_merged.groupby(['Qname','Nth_hit'],as_index=False).diff_e.min()\n",
    "    df_merged_t=pd.merge(df_merged,tmp1,how='inner')\n",
    "    df_start=df_merged_t[['Qname','Nth_hit','exstart','diff_s']].drop_duplicates()\n",
    "    df_merged_t=pd.merge(df_merged,tmp2,how='inner')\n",
    "    df_end=df_merged_t[['Qname','Nth_hit','exend','diff_e']].drop_duplicates()\n",
    "    df_s_e=pd.merge(df_start,df_end)\n",
    "    df_out=pd.merge(df,df_s_e).sort_values(['Qname','Nth_hit'])\n",
    "    df_out['diff_s']=df_out['Tstart']-df_out['exstart']\n",
    "    df_out['diff_e']=df_out['exend']-df_out['Tend']\n",
    "    return df_out\n",
    "\n",
    "def fix_start_end(df_in):\n",
    "    df=df_in.copy()\n",
    "    df[['Qstart_fix','Qend_fix','Tstart_fix','Tend_fix']]=df[['Qstart','Qend','Tstart','Tend']].copy()\n",
    "    #Nth1 dir +\n",
    "    df.loc[(df['Nth_hit']==1) & (df['dir']==\"+\"),\"Tend_fix\"]=df['exend']\n",
    "    df.loc[(df['Nth_hit']==1) & (df['dir']==\"+\"),'Qend_fix']=df['Qend']+df['diff_e']\n",
    "    #Nth1 dir -\n",
    "    df.loc[(df['Nth_hit']==1) & (df['dir']==\"-\"),\"Tstart_fix\"]=df['exstart']\n",
    "    df.loc[(df['Nth_hit']==1) & (df['dir']==\"-\"),'Qend_fix']=df['Qend']+df['diff_s']\n",
    "    #df.loc[(df['Nth_hit']==1) & (df['dir']==\"-\")]\n",
    "\n",
    "    #Nth2 dir +\n",
    "    df.loc[(df['Nth_hit']==2) & (df['dir']==\"+\"),\"Tstart_fix\"]=df['exstart']\n",
    "    df.loc[(df['Nth_hit']==2) & (df['dir']==\"+\"),'Qstart_fix']=df['Qstart']-df['diff_s']\n",
    "    #Nth1 dir -\n",
    "    df.loc[(df['Nth_hit']==2) & (df['dir']==\"-\"),\"Tend_fix\"]=df['exend']\n",
    "    df.loc[(df['Nth_hit']==2) & (df['dir']==\"-\"),'Qstart_fix']=df['Qstart']-df['diff_e']\n",
    "    return df\n",
    "            \n",
    "\n",
    "def judge_cross_over_v3(mmap2_paired,buffer,s,e):\n",
    "    ##print('クエリ側のクロスオーバーの判定')\n",
    "    rids=[]\n",
    "    if mmap2_paired['Nth_hit'].value_counts()[1] != mmap2_paired['Nth_hit'].value_counts()[2]:\n",
    "        drop_rid_nopaired = pd.DataFrame(mmap2_paired.groupby('Qname')['Nth_hit'].nunique() == 1).query('Nth_hit==True').index\n",
    "        mmap2_paired = mmap2_paired.query('Qname not in @drop_rid_nopaired')\n",
    "    for x,rid in enumerate(tqdm(mmap2_paired.Qname.unique())): #read_idごと\n",
    "        mmap2_paired_sub = mmap2_paired.query('Qname==@rid')\n",
    "        N=mmap2_paired_sub.Nth_hit.min()\n",
    "        first_end_pos = int(mmap2_paired_sub.query('Nth_hit==@N')[e].max())\n",
    "        try:\n",
    "            second_start_pos = int(mmap2_paired_sub.query('Nth_hit==@N+1')[s].min())\n",
    "        except:\n",
    "            assert mmap2_paired_sub['Nth_hit'].nunique()==2 ,'Nth_hitが3以上です'\n",
    "            second_start_pos = int(mmap2_paired_sub.query('Nth_hit!=@N')[s].min())\n",
    "        if first_end_pos - buffer > second_start_pos:\n",
    "            rids.append(rid)\n",
    "    return rids\n",
    "    \n",
    "def split_bp(bp):\n",
    "    chr1=bp.split(\"___\")[0].split(\"__\")[0]\n",
    "    pos1=int(bp.split(\"___\")[0].split(\"__\")[1])\n",
    "    chr2=bp.split(\"___\")[1].split(\"__\")[0]\n",
    "    pos2=int(bp.split(\"___\")[1].split(\"__\")[1])\n",
    "    return pd.Series(list([chr1,pos1,chr2,pos2]))\n",
    "\n",
    "def make_bp_clst_mode(df):\n",
    "    df[[\"name2_chr1\",\"name2_pos1\",\"name2_chr2\",\"name2_pos2\"]]=df['name2'].apply(lambda x:split_bp(x))\n",
    "    df['g1_g2_clst'] = df['g1_clst'] + \"--\" + df[\"g2_clst\"]\n",
    "    \n",
    "    tmp=pd.DataFrame(df.groupby('g1_g2_clst',as_index=True)['name2_pos1'].apply(lambda x: x.mode())).reset_index().drop('level_1',axis=1).rename(columns={'name2_pos1':'name2_pos1_mode'})\n",
    "    tmp2=pd.DataFrame(df.groupby('g1_g2_clst',as_index=True)['name2_pos2'].apply(lambda x: x.mode())).reset_index().drop('level_1',axis=1).rename(columns={\"name2_pos2\":'name2_pos2_mode'})\n",
    "    df_bp_mode=pd.merge(tmp,tmp2)\n",
    "    df=pd.merge(df,df_bp_mode,how='left',on='g1_g2_clst')\n",
    "    df[\"name2_clst\"]=df['name2_chr1']+\"__\"+df['name2_pos1_mode'].astype(str)+\"___\"+df['name2_chr2']+\"__\"+df['name2_pos2_mode'].astype(str)\n",
    "    df=df.drop(['name2_chr1','name2_chr2','name2_pos1','name2_pos2'],axis=1)\n",
    "    return df\n",
    "\n",
    "def cul_gaplen(df_in,Qstart,Qend):\n",
    "    df = df_in[['Qname','Qlen' ,Qstart, Qend,'Nth_hit']].drop_duplicates()\n",
    "    res_df=pd.DataFrame()\n",
    "    for rid in tqdm(df.Qname.unique()):\n",
    "        df_sub = df.query('Qname in @rid')\n",
    "        Nths = df_sub.Nth_hit.nunique()\n",
    "        if Nths==2: \n",
    "            df_sub1=df_sub.query('Nth_hit == 1')\n",
    "            df_sub2=df_sub.query('Nth_hit == 2')\n",
    "            N=0\n",
    "            for (_,r1) in df_sub1.iterrows():\n",
    "                for (_,r2) in df_sub2.iterrows():\n",
    "                    res = pd.Series([rid, int(r2[Qstart])-int(r1[Qend])],index=['rid','gaplen'])\n",
    "                    res_df = res_df.append(res,ignore_index=True)\n",
    "                    N=N+1\n",
    "    return res_df\n",
    "\n",
    "def bp_update_v3(df_in):\n",
    "    df=df_in.copy()\n",
    "    df['Tstart_exon']=df['Tstart']\n",
    "    df['Tend_exon']=df['Tend']\n",
    "    df.loc[(df['gene_gap_equal']==1)&(df[\"Nth_hit\"]==1)&(df[\"dir\"]==\"+\"),\"Tend\"]=df[\"Tend_gap\"]\n",
    "    df.loc[(df['gene_gap_equal']==1)&(df[\"Nth_hit\"]==1)&(df[\"dir\"]==\"-\"),\"Tstart\"]=df[\"Tstart_gap\"]\n",
    "    df.loc[(df['gene_gap_equal']==1)&(df[\"Nth_hit\"]==2)&(df[\"dir\"]==\"-\"),\"Tend\"]=df[\"Tend_gap\"]\n",
    "    df.loc[(df['gene_gap_equal']==1)&(df[\"Nth_hit\"]==2)&(df[\"dir\"]==\"+\"),\"Tstart\"]=df[\"Tstart_gap\"]\n",
    "    return df\n",
    "\n",
    "def flging_gap_use_read_v2(df):\n",
    "    Qnames=df.query('(gene_gap.notna()) and( gene in gene_gap or gene_gap in gene)').Qname.unique()\n",
    "    #print(len(Qnames))\n",
    "    df['gap_use']=0\n",
    "    df.loc[df['Qname'].isin(Qnames),'gap_use']=1\n",
    "    return df\n",
    "def check_gene_genegap(gene,gene_gap):\n",
    "    if pd.isna(gene) or pd.isna(gene_gap):\n",
    "        return 0\n",
    "    elif gene in gene_gap or gene_gap in gene:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def make_bp_clst_mode_v2(df):\n",
    "    df[[\"name2_chr1\",\"name2_pos1\",\"name2_chr2\",\"name2_pos2\"]]=df['name2'].apply(lambda x:split_bp(x))\n",
    "    tmp1=df.groupby('g1_g2_clst',as_index=True)['name2_pos1'].value_counts().to_frame().rename(columns={\"name2_pos1\":\"name2_pos1_count\"}).reset_index().rename(columns={'name2_pos1':'name2_pos1_mode'})#.drop_duplicates(\"g1_g2_clst\")\n",
    "    tmp2=df.groupby('g1_g2_clst',as_index=True)['name2_pos2'].value_counts().to_frame().rename(columns={\"name2_pos2\":\"name2_pos2_count\"}).reset_index().rename(columns={'name2_pos2':'name2_pos2_mode'})#.drop_duplicates(\"g1_g2_clst\")\n",
    "    tmp3=pd.merge(tmp1.groupby('g1_g2_clst')[\"name2_pos1_mode\"].apply(list).to_frame().reset_index(),tmp1.groupby('g1_g2_clst')[\"name2_pos1_count\"].apply(list).to_frame().reset_index()).rename(columns={'name2_pos1_mode':'name2_pos1_mode_list','name2_pos1_count':'name2_pos1_count_list'})\n",
    "    tmp4=pd.merge(tmp2.groupby('g1_g2_clst')[\"name2_pos2_mode\"].apply(list).to_frame().reset_index(),tmp2.groupby('g1_g2_clst')[\"name2_pos2_count\"].apply(list).to_frame().reset_index()).rename(columns={'name2_pos2_mode':'name2_pos2_mode_list','name2_pos2_count':'name2_pos2_count_list'})\n",
    "    df_bp_mode=pd.merge(tmp1.drop_duplicates(\"g1_g2_clst\"),pd.merge(tmp2.drop_duplicates(\"g1_g2_clst\"),pd.merge(tmp3,tmp4)))\n",
    "    df=pd.merge(df,df_bp_mode,how='left',on='g1_g2_clst')\n",
    "    df[\"name2_clst\"]=df['name2_chr1']+\"__\"+df['name2_pos1_mode'].astype(str)+\"___\"+df['name2_chr2']+\"__\"+df['name2_pos2_mode'].astype(str)\n",
    "    df=df.drop(['name2_chr1','name2_chr2','name2_pos1','name2_pos2'],axis=1)\n",
    "    return df\n",
    "\n",
    "def bp_update_v4(df_in):\n",
    "    df=df_in.copy()\n",
    "    #df['Tstart_exon']=df['Tstart']\n",
    "    #df['Tend_exon']=df['Tend']\n",
    "    df['gap_bp_hosei']=0\n",
    "    df.loc[(abs(df['Tstart_gap']-df['exstart'])<=buffer) | (abs(df['Tstart_gap']-df['exend'])<=buffer),\"gap_bp_hosei\"]=1\n",
    "    df.loc[(abs(df['Tend_gap']-df['exstart'])<=buffer) | (abs(df['Tend_gap']-df['exend'])<=buffer),\"gap_bp_hosei\"]=1\n",
    "    df.loc[(df['gap_bp_hosei']==1)&(df['gene_gap_equal']==1)&(df[\"Nth_hit\"]==1)&(df[\"dir\"]==\"+\"),\"Tend\"]=df[\"exend\"]\n",
    "    df.loc[(df['gap_bp_hosei']==1)&(df['gene_gap_equal']==1)&(df[\"Nth_hit\"]==1)&(df[\"dir\"]==\"-\"),\"Tstart\"]=df[\"exstart\"]\n",
    "    df.loc[(df['gap_bp_hosei']==1)&(df['gene_gap_equal']==1)&(df[\"Nth_hit\"]==2)&(df[\"dir\"]==\"-\"),\"Tend\"]=df[\"exend\"]\n",
    "    df.loc[(df['gap_bp_hosei']==1)&(df['gene_gap_equal']==1)&(df[\"Nth_hit\"]==2)&(df[\"dir\"]==\"+\"),\"Tstart\"]=df[\"exstart\"]\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### edgeアライメント準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "234it [00:00, 678.66it/s]\n",
      "100%|██████████| 167/167 [00:00<00:00, 268.43it/s]\n",
      "100%|██████████| 116/116 [00:00<00:00, 241.92it/s]\n",
      "100%|██████████| 68/68 [00:00<00:00, 291.72it/s]\n"
     ]
    }
   ],
   "source": [
    "root = Path.cwd().parent\n",
    "# remote_file_dir = Path(\"/raidc/keigo.masuda/analysis/minimap2/MCF7/FLBEA/\")\n",
    "#input\n",
    "paf_dir = root / f\"out/\" \n",
    "paffile_name = f\"{TARGET}_hg19.paf\"\n",
    "paffile_name_refseq=f\"{TARGET}_refseq.paf\"\n",
    "paffile_path = paf_dir /paffile_name\n",
    "paffile_refseq_path = paf_dir/paffile_name_refseq\n",
    "\n",
    "#outdir\n",
    "res_file_dir = root / f\"out/\" \n",
    "intermediate_file_dir = res_file_dir / f\"intermediate\"\n",
    "res_genome_dir = res_file_dir / f\"genome\"\n",
    "res_genome_path = res_genome_dir / f\"{TARGET}_df_fusioncand_genome.csv\"\n",
    "res_refseq_path = res_genome_dir / f\"{TARGET}_df_fusioncand_refseq.csv\"\n",
    "res_4_edge_alignment_dir = res_file_dir / \"for_edge_alignment/\"\n",
    "for_make_edge_file_path = res_4_edge_alignment_dir / f\"{TARGET}/gap_4make_edge.csv\"\n",
    "\n",
    "#gtf\n",
    "gtf_path = root / \"data/ref/hg19_genCode19.tab.usecol\" #refseqだけにしない 遺伝子のstart endのみ。エキソンごとではない\n",
    "gtf_exon_path = root / \"data/ref/hg19_genCode19.tab.exon\" #refseqだけにしない 遺伝子のstart endのみ。エキソンごとではない\n",
    "\n",
    "\n",
    "mmap2_align2genome_path =  intermediate_file_dir / f\"{TARGET}_genome.csv\"\n",
    "\n",
    "\n",
    "### 結果保存用 dir作成\n",
    "if not os.path.exists(res_file_dir):\n",
    "    !mkdir -p $res_file_dir\n",
    "    !mkdir -p $res_file_dir/'intermediate'\n",
    "    !mkdir -p $res_file_dir/'genome'\n",
    "    !mkdir -p $res_file_dir/'for_edge_alignment'\n",
    "\n",
    "if not os.path.exists(f\"{res_file_dir}/for_edge_alignment/{TARGET}\"):\n",
    "    !mkdir -p $res_file_dir/'for_edge_alignment'/$TARGET\n",
    "\n",
    "\n",
    "#gtfファイル読み込み\n",
    "gtf = prep_gtf(pd.read_csv(gtf_path))\n",
    "\n",
    "#refseqの処理-----------------------------------------------------------\n",
    "#refseqに1geneにしか当たらないリードは除外\n",
    "multihit_id,mmap2_refseq_org, mmap2_refseq = filter_refseq_paf(paffile_refseq_path, gtf_path)  \n",
    "\n",
    "#genomeの処理-----------------------------------------------------------\n",
    "mmap2_org,mmap2 = prep_paf_file_v3(paffile_path, multihit_id, 1)\n",
    "\n",
    "#Nth=2を採用\n",
    "mmap2_Nth2 = filter_out_Nth_hit_ov2(mmap2)\n",
    "\n",
    "\n",
    "# ### クエリ側のクロスオーバーの判定とクロスオーバーの除去\n",
    "cross_over_rids = judge_cross_over_v3(mmap2_Nth2,15,'Qstart','Qend')\n",
    "mmap2_Nth2_rmcross = mmap2_Nth2.query('Qname not in @cross_over_rids')\n",
    "\n",
    "#遺伝子を判定 \n",
    "mmap2_Nth2_rmcross['gene']=mmap2_Nth2_rmcross.apply(lambda x: get_gene_name_start_end(gtf, x['Tname'],x['Tstart'],x['Tend']), axis=1)\n",
    "drop_rid=mmap2_Nth2_rmcross[mmap2_Nth2_rmcross.gene==\"intron\"].Qname\n",
    "mmap2_Nth2_rmcross=mmap2_Nth2_rmcross.query('Qname not in @drop_rid')\n",
    "\n",
    "# 重複削除 Qname,Nth_hit,geneがユニークになる様に　クエリ側が多くヒットした配列を優先\n",
    "mmap2_Nth2_rmcross_rmdup=mmap2_Nth2_rmcross.sort_values(['Qname','Nth_hit','Qhit'],ascending=[True,True,False]).drop_duplicates(subset=['Qname','Nth_hit'])\n",
    "\n",
    "#エキソンの端の調整\n",
    "mmap2_Nth2_rmcross_rmdup_addex=add_exon_s_e(mmap2_Nth2_rmcross_rmdup,gtf_exon_path)\n",
    "mmap2_Nth2_rmcross_rmdup_addex=fix_start_end(mmap2_Nth2_rmcross_rmdup_addex)\n",
    "mmap2_Nth2_rmcross_rmdup_addex=mmap2_Nth2_rmcross_rmdup_addex.drop(['gene2'],axis=1).drop_duplicates()\n",
    "\n",
    "# ### クエリ側のクロスオーバーの判定とクロスオーバーの除去\n",
    "cross_over_rids = judge_cross_over_v3(mmap2_Nth2_rmcross_rmdup_addex,15,'Qstart_fix','Qend_fix')\n",
    "mmap2_Nth2_rmcross_rmdup_addex_rmcross=mmap2_Nth2_rmcross_rmdup_addex.query('Qname not in @cross_over_rids')\n",
    "\n",
    "\n",
    "edge_start_end=prep_subseq_read_gaponly_v2(mmap2_Nth2_rmcross_rmdup_addex_rmcross.drop_duplicates(subset=['Qname','Qstart_fix','Qend_fix']))\n",
    "edge_start_end['length']=edge_start_end['Qend']-edge_start_end['Qstart']+1\n",
    "edge_start_end_ov=edge_start_end.query('length>@realign_gap_len_criteria').sort_values('length',ascending=False).drop_duplicates(subset='Qname')\n",
    "\n",
    "#fileの書き出し------------------------------------------------------------\n",
    "mmap2_Nth2_rmcross_rmdup_addex_rmcross.to_csv(mmap2_align2genome_path, index=False)\n",
    "edge_start_end_ov=edge_start_end_ov.query('Qstart>=0')\n",
    "edge_start_end_ov=pd.merge(edge_start_end_ov,mmap2_Nth2_rmcross_rmdup_addex_rmcross[['Qname','Qlen']]).query('length<Qlen').drop('Qlen',axis=1).drop_duplicates()\n",
    "edge_start_end_ov['Qstart']=edge_start_end_ov.Qstart.astype(int)\n",
    "edge_start_end_ov['Qend']=edge_start_end_ov.Qend.astype(int)\n",
    "edge_start_end_ov.to_csv(for_make_edge_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "215bd58fd8531144f9200bfb3bbe6bdfea4498d65455a6613b4bae8dff07dfad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
