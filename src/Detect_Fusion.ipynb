{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET=\"testdata\"\n",
    "mapping_rate_criteria_trans=0.99\n",
    "buffer=15\n",
    "Nth_hit_buffer=50\n",
    "unpaired_mapping_rate=0.95\n",
    "mapping_gap_rate=0.5\n",
    "mapping_gap_len_criteria=10000\n",
    "clst_percent_cutoff=0.5\n",
    "clst_bp=1000\n",
    "\n",
    "realign_gap_len_criteria=15\n",
    "gap_len_cutoff=15\n",
    "\n",
    "gap_mapping_rate_cutoff=0.5\n",
    "gap_evalue_cutoff=0.05\n",
    "buffer=15"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 関数のインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from glob import glob \n",
    "#import matplotlib.pyplot as plt\n",
    "#import collections\n",
    "#mport pandas_bj\n",
    "#sns.set_context('notebook')\n",
    "#!jupyter nbextension enable --py --sys-prefix widgetsnbextension\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "# pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_round_int(x): return int((x * 2 + 1) // 2)\n",
    "\n",
    "def rm_cross_over(df_fusioncand,buffer=buffer):\n",
    "    df_A = df_fusioncand[df_fusioncand[\"chr1\"] !=df_fusioncand[\"chr2\"]]  # 同じ染色体ではないのでクロスオーバーの対象外\n",
    "    tmp = df_fusioncand[df_fusioncand[\"chr1\"] == df_fusioncand[\"chr2\"]]\n",
    "    drop1 = tmp.query('start1<start2 and end1>end2')\n",
    "    drop2 = tmp.query('start1>start2 and end1<end2')\n",
    "    drop_rid=list(drop1['hit_rid'])+list(drop1['hit_rid'])\n",
    "\n",
    "    df_B_a = tmp[(tmp[\"end1\"] - tmp[\"start2\"] - buffer) *\n",
    "                 (tmp[\"end2\"] - tmp[\"start1\"] - buffer) < 0]\n",
    "    df_B_b = tmp[(tmp[\"end1\"] - tmp[\"start2\"] - buffer) *\n",
    "                 (tmp[\"end2\"] - tmp[\"start1\"] - buffer) < 0]\n",
    "\n",
    "    df_fusioncand_rmcross = pd.concat([df_A, df_B_a, df_B_b])\n",
    "    df_fusioncand_rmcross = df_fusioncand_rmcross.drop_duplicates()\n",
    "    df_fusioncand_rmcross.query('hit_rid not in @drop_rid')\n",
    "    #print('クロスオーバーで除く本数は{}本'.format(df_fusioncand.shape[0] - df_fusioncand_rmcross.shape[0]))\n",
    "    return df_fusioncand_rmcross,drop_rid\n",
    "\n",
    "def judge_cross_over_v3(mmap2_paired,buffer,s,e):\n",
    "    ##print('クエリ側のクロスオーバーの判定')\n",
    "    rids=[]\n",
    "    if mmap2_paired['Nth_hit'].value_counts()[1] != mmap2_paired['Nth_hit'].value_counts()[2]:\n",
    "        drop_rid_nopaired = pd.DataFrame(mmap2_paired.groupby('Qname')['Nth_hit'].nunique() == 1).query('Nth_hit==True').index\n",
    "        mmap2_paired = mmap2_paired.query('Qname not in @drop_rid_nopaired')\n",
    "    for x,rid in enumerate(tqdm(mmap2_paired.Qname.unique())): #read_idごと\n",
    "        mmap2_paired_sub = mmap2_paired.query('Qname==@rid')\n",
    "        N=mmap2_paired_sub.Nth_hit.min()\n",
    "        first_end_pos = int(mmap2_paired_sub.query('Nth_hit==@N')[e].max())\n",
    "        try:\n",
    "            second_start_pos = int(mmap2_paired_sub.query('Nth_hit==@N+1')[s].min())\n",
    "        except:\n",
    "            assert mmap2_paired_sub['Nth_hit'].nunique()==2 ,'Nth_hitが3以上です'\n",
    "            second_start_pos = int(mmap2_paired_sub.query('Nth_hit!=@N')[s].min())\n",
    "        if first_end_pos - buffer > second_start_pos:\n",
    "            rids.append(rid)\n",
    "    return rids\n",
    "    \n",
    "\n",
    "\n",
    "def cul_mapping_rate_all(df):\n",
    "    df_tmp = df.copy()\n",
    "    df_tmp['map_len'] = df_tmp.apply(lambda x: x['Qend']-x['Qstart'], axis=1)\n",
    "    df_tmp=df_tmp.drop_duplicates(['Qname','Nth_hit'])\n",
    "    mapping_rate_all = df_tmp.groupby(['Qname']).agg({'map_len': 'sum', 'Qlen': 'max'}).apply(lambda x: x['map_len']/x['Qlen'], axis=1)\n",
    "    add_df = pd.DataFrame(mapping_rate_all, columns=['mapping_rate_all']).reset_index()\n",
    "    df_out = pd.merge(df, add_df)\n",
    "    return df_out\n",
    "\n",
    "def cul_mapping_GAP(df):\n",
    "    df_tmp = df.copy()\n",
    "    df_tmp['map_len'] = df_tmp.apply(lambda x: x['Qend']-x['Qstart'], axis=1)\n",
    "    df_tmp=df_tmp.drop_duplicates(['Qname','Nth_hit'])\n",
    "    mapping_gap = df_tmp.groupby(['Qname']).agg({'Qend': 'min', 'Qstart': 'max'}).apply(lambda x: x['Qstart']-x['Qend'], axis=1)\n",
    "    add_df = pd.DataFrame(mapping_gap, columns=['mapping_gap']).reset_index()\n",
    "    df_out = pd.merge(df, add_df)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def get_gene_name_start_end(gtf,chr,s,e,buffer=15):\n",
    "    gtf['len']=gtf['end']-gtf['start']\n",
    "    multi_hit_flg=0\n",
    "    gtf_sub1=gtf.query('chr==@chr & start-@buffer <= @s <= end + @buffer and chr==@chr & start-@buffer <= @e <= end+@buffer') #gtfの中にstart,endともに張り付いている\n",
    "    if len(gtf_sub1) >= 1:\n",
    "        gtf_sub1= gtf_sub1.sort_values('len',ascending=False)\n",
    "        g_tmp = gtf_sub1['gene'].unique()\n",
    "        g_multi=\"||\".join(g_tmp)\n",
    "        #g=g_tmp[0]\n",
    "        if len(g_tmp) > 1:\n",
    "            multi_hit_flg=1\n",
    "            g_multi=\"||\".join(g_tmp)\n",
    "    else:\n",
    "        gtf_sub2=gtf.query('chr==@chr & start-@buffer <= @s <= end+@buffer') #gtfの中にstartのみ張り付いている\n",
    "        gtf_sub3=gtf.query('chr==@chr & start-@buffer <= @e <= end+@buffer') #gtfの中にendのみ張り付いている\n",
    "        if len(gtf_sub2)>=1: \n",
    "            gtf_sub2= gtf_sub2.sort_values('len',ascending=False)\n",
    "            g_tmp = gtf_sub2['gene'].unique()\n",
    "            g_multi=\"||\".join(g_tmp)\n",
    "            #g=g_tmp[0]\n",
    "            if len(g_tmp) > 1:\n",
    "                multi_hit_flg=1\n",
    "        elif len(gtf_sub3)>=1: \n",
    "            gtf_sub3= gtf_sub3.sort_values('len',ascending=False)\n",
    "            g_tmp = gtf_sub3['gene'].unique()\n",
    "            g_multi=\"||\".join(g_tmp)\n",
    "            #g=g_tmp[0]\n",
    "            if len(g_tmp)>1:\n",
    "                multi_hit_flg=1\n",
    "        else:\n",
    "            #g='no_hit'\n",
    "            g_multi='intron'\n",
    "        \n",
    "    return pd.Series(g_multi)\n",
    "\n",
    "def prep_gtf(gtf):\n",
    "    gtf['gene_3char'] = gtf['name2'].str[0:3]\n",
    "    gtf = gtf.rename(columns={\"name2\": \"gene\", 'chrom': 'chr','txStart': 'start', 'txEnd': 'end'}).sort_values(['chr', 'start'])\n",
    "    return gtf\n",
    "\n",
    "def add_Nth_hit(mmap2_paired_in,X=Nth_hit_buffer): #30から変更\n",
    "    ##print('Nth hitの判定')\n",
    "    mmap2_paired = mmap2_paired_in.sort_values(['Qname','Qstart','Qend','mapQ','match_rate'],ascending=[True,True,True,False,False])\n",
    "    res=[]\n",
    "    for x,rid in tqdm(enumerate(mmap2_paired.Qname.unique())): #read_idごと\n",
    "        counter=1 #Nth\n",
    "        mmap2_paired_sub = mmap2_paired.query('Qname==@rid')\n",
    "        for n,(_,r) in enumerate(mmap2_paired_sub.iterrows()): #同じread_idでの複数ヒット\n",
    "            if n==0:\n",
    "                res.append(1)\n",
    "            else:\n",
    "                if (mmap2_paired_sub.iloc[n-1][\"Qstart\"] >= mmap2_paired_sub.iloc[n]['Qstart'] - X):\n",
    "                    counter+=0\n",
    "                    res.append(counter)\n",
    "                elif (mmap2_paired_sub.iloc[n-1][\"Qend\"] >= mmap2_paired_sub.iloc[n]['Qend'] -X ): \n",
    "                    counter+=0\n",
    "                    res.append(counter)\n",
    "                else:\n",
    "                    counter+=1\n",
    "                    res.append(counter)\n",
    "    mmap2_paired[\"Nth_hit\"] = res\n",
    "    return mmap2_paired\n",
    "\n",
    "def filter_out_Nth_hit_ov2(df):\n",
    "    cond = df.groupby(\"Qname\").Nth_hit.max() == 2\n",
    "    filterd_qid = cond[cond].index\n",
    "    df_out=df.query('Qname in @filterd_qid')\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def filter_out_mismatch_clst_gname(clst,g1,g2):\n",
    "    clst_1=clst.split(\"--\")[0]\n",
    "    clst_2=clst.split(\"--\")[1]\n",
    "    if (clst_1 in g1 or clst_2 in g1) and (clst_1 in g2 or clst_2 in g2):\n",
    "        ##print(clst,g1,g2,\"_clst_geme_mismatch\")\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def cul_fusioncand_v4_4refseq(mmap2_paired):\n",
    "    ##print('---------------融合点の計算----------------')\n",
    "    col = [\"Qname\",\"Qstart\",\"Qend\",\"dir\", \"Tname\", \"Tstart\", \"Tend\", \"mapQ\",\"match_rate\", \"mapping_rate\",\"gene\"]\n",
    "    out_col = [\"hit_rid\", \"Qstart1\",\"Qend1\",\"dir1\", \"chr1\", \"start1\", \"end1\", \"mapQ1\", \"match_rate1\", \"mapping_rate1\",\"g1\",\"Qstart2\",\"Qend2\",\"dir2\", \"chr2\",\n",
    "               \"start2\", \"end2\", \"mapQ2\", \"match_rate2\", 'mapping_rate2', \"g2\",\"gap_len\",'gap_rate']\n",
    "    arr=[]\n",
    "    for rid in tqdm(mmap2_paired.Qname.unique()):\n",
    "        mmap2_paired_sub = mmap2_paired.query('Qname==@rid')\n",
    "        if mmap2_paired_sub.Nth_hit.nunique()==2:\n",
    "            mmap2_paired_sub_Nth1= mmap2_paired_sub.query('Nth_hit==1')\n",
    "            mmap2_paired_sub_Nth2= mmap2_paired_sub.query('Nth_hit==2')\n",
    "            for i in range(len(mmap2_paired_sub_Nth1)):\n",
    "                for j in range(len(mmap2_paired_sub_Nth2)):\n",
    "                    flont_fp = list(mmap2_paired_sub_Nth1.iloc[i][col]) \n",
    "                    rear_fp = list(mmap2_paired_sub_Nth2.iloc[j][col])[1:]\n",
    "                    gap_len =  mmap2_paired_sub_Nth2.iloc[j]['Qstart']- mmap2_paired_sub_Nth1.iloc[i]['Qend']\n",
    "                    gap_rate =  gap_len / mmap2_paired_sub_Nth1.iloc[i]['Qlen']\n",
    "                    res = flont_fp + rear_fp + [gap_len] + [gap_rate] \n",
    "                    arr.append(res)\n",
    "                    \n",
    "        else:\n",
    "            pass\n",
    "    df_fusioncand=pd.DataFrame(arr)\n",
    "    df_fusioncand.columns=out_col\n",
    "    df_fusioncand = df_fusioncand.drop_duplicates()\n",
    "    return df_fusioncand\n",
    "\n",
    "def cul_fusioncand_v4_quick(mmap2_paired):\n",
    "    ##print('---------------融合点の計算----------------')\n",
    "    col = [\"Qname\",\"Qstart\",\"Qend\",\"dir\", \"Tname\", \"Tstart\", \"Tend\", \"mapQ\",\"match_rate\", \"mapping_rate\"]\n",
    "    out_col = [\"hit_rid\", \"Qstart1\",\"Qend1\",\"dir1\", \"chr1\", \"start1\", \"end1\", \"mapQ1\", \"match_rate1\", \"mapping_rate1\",\"Qstart2\",\"Qend2\",\"dir2\", \"chr2\",\n",
    "               \"start2\", \"end2\", \"mapQ2\", \"match_rate2\", 'mapping_rate2', \"gap_len\",'gap_rate']\n",
    "    arr=[]\n",
    "    for rid in tqdm(mmap2_paired.Qname.unique()):\n",
    "        mmap2_paired_sub = mmap2_paired.query('Qname==@rid')\n",
    "        if mmap2_paired_sub.Nth_hit.nunique()==2:\n",
    "            mmap2_paired_sub_Nth1= mmap2_paired_sub.query('Nth_hit==1')\n",
    "            mmap2_paired_sub_Nth2= mmap2_paired_sub.query('Nth_hit==2')\n",
    "            for i in range(len(mmap2_paired_sub_Nth1)):\n",
    "                for j in range(len(mmap2_paired_sub_Nth2)):\n",
    "                    flont_fp = list(mmap2_paired_sub_Nth1.iloc[i][col]) \n",
    "                    rear_fp = list(mmap2_paired_sub_Nth2.iloc[j][col])[1:]\n",
    "                    gap_len =  mmap2_paired_sub_Nth2.iloc[j]['Qstart']- mmap2_paired_sub_Nth1.iloc[i]['Qend']\n",
    "                    gap_rate =  gap_len / mmap2_paired_sub_Nth1.iloc[i]['Qlen']\n",
    "                    res = flont_fp + rear_fp + [gap_len] + [gap_rate] \n",
    "                    arr.append(res)\n",
    "                    \n",
    "        else:\n",
    "            pass\n",
    "            ### 3箇所以上にマッピングされるものは候補から除外\n",
    "        df_fusioncand=pd.DataFrame(arr)\n",
    "        df_fusioncand.columns=out_col\n",
    "        df_fusioncand = df_fusioncand.drop_duplicates()\n",
    "    return df_fusioncand\n",
    "\n",
    "\n",
    "def prep_paf_edge_4gaponly(path):\n",
    "    paf_col=[\"Qname\", \"Qlen\", \"Qstart\", \"Qend\", \"dir\", \"Tname\", \"Tlen\", \"Tstart\", \"Tend\", \"match\", \"block\", \"mapQ\"]\n",
    "    df = pd.read_table(path, index_col=False, header=None, names=paf_col, usecols=range(0, 12)).drop('Tlen',axis=1)\n",
    "    if len(df)>0:\n",
    "        #df[\"Nth_hit\"]=df['Qname'].str.rsplit(\"_\",2).str[1].str[0].astype(int)\n",
    "        df[\"pat\"]=df['Qname'].str.rsplit(\"_\",1).str[-1].str[0]\n",
    "        df['Qname'] = df['Qname'].str.split(',').str[0].str.rsplit(\"_\",4).str[0]\n",
    "        #df['gene'] = df.apply(lambda x: get_gene_name_from_rid(x['Qname']), axis=1)\n",
    "        df['match_rate'] = (df['match'])/(df['Qend']-df['Qstart']+1)\n",
    "        df['mapping_rate'] = (df['Qend']-df['Qstart']+1)/df['Qlen']\n",
    "        df.sort_values(['Qname','Qstart'],ascending=[True,True],inplace=True)\n",
    "        return df\n",
    "\n",
    "def prep_subseq_read_gaponly(df_in):\n",
    "    res_df=pd.DataFrame()\n",
    "    ##print('----start prep_subseq_read gap only -------')\n",
    "    df = df_in[['Qname','Qlen' ,'Qstart', 'Qend','Nth_hit']].drop_duplicates()\n",
    "    out_col=['Qname', 'Qstart', 'Qend']\n",
    "    for rid in tqdm(df.Qname.unique()):\n",
    "        df_sub = df.query('Qname in @rid')\n",
    "        Nths = df_sub.Nth_hit.nunique()\n",
    "        if Nths==2: \n",
    "            df_sub1=df_sub.query('Nth_hit == 1')\n",
    "            df_sub2=df_sub.query('Nth_hit == 2')\n",
    "            N=0\n",
    "            for (_,r1) in df_sub1.iterrows():\n",
    "                for (_,r2) in df_sub2.iterrows():\n",
    "                    res = pd.Series([rid, int(r1['Qend']),int(r2['Qstart'])],index=out_col)\n",
    "                    res_df = res_df.append(res,ignore_index=True)\n",
    "                    N=N+1\n",
    "\n",
    "    res_df = res_df.astype({'Qend':int,'Qstart':int}).sort_values(['Qname','Qstart','Qend'])[out_col].drop_duplicates(subset=['Qname','Qstart','Qend'])\n",
    "    assert res_df.Qname.nunique()==df.Qname.nunique()\n",
    "    return res_df\n",
    "\n",
    "def filter_only_2pairofgene(df_fusioncand_in,g1_col,g2_col):\n",
    "    df_fusioncand = df_fusioncand_in.copy()\n",
    "    cond1=df_fusioncand.groupby('hit_rid')[g1_col].nunique()==1\n",
    "    g1_uniq_rid=cond1[cond1].index\n",
    "    cond2=df_fusioncand.groupby('hit_rid')[g2_col].nunique()==1\n",
    "    g2_uniq_rid=cond2[cond2].index\n",
    "    g1_g2_uniq_rid=set(g1_uniq_rid)&set(g2_uniq_rid)\n",
    "    df_fusioncand = df_fusioncand.query('hit_rid in @g1_g2_uniq_rid')\n",
    "    return df_fusioncand\n",
    "\n",
    "\n",
    "def reclst_stop(df_in,dont_reclust_cutoff):\n",
    "    df_fusioncand=df_in.copy()\n",
    "    df_fusioncand.loc[(df_fusioncand['clst_count_1st']>=dont_reclust_cutoff) & (df_fusioncand[\"clst_final\"]!=df_fusioncand['clst_1st']),'g1_clst']=df_fusioncand['g1_clst_1st']\n",
    "    df_fusioncand.loc[(df_fusioncand['clst_count_1st']>=dont_reclust_cutoff) & (df_fusioncand[\"clst_final\"]!=df_fusioncand['clst_1st']),'g2_clst']=df_fusioncand['g2_clst_1st']\n",
    "    df_fusioncand.loc[(df_fusioncand['clst_count_1st']>=dont_reclust_cutoff) & (df_fusioncand[\"clst_final\"]!=df_fusioncand['clst_1st']),'clst_final']=df_fusioncand['clst_1st']\n",
    "    df_fusioncand.loc[(df_fusioncand['clst_count_1st']>=dont_reclust_cutoff) & (df_fusioncand[\"clst_final\"]!=df_fusioncand['clst_1st']),'clst_count']=df_fusioncand['clst_count_1st']\n",
    "    df_fusioncand.loc[(df_fusioncand['clst_count_1st']>=dont_reclust_cutoff) & (df_fusioncand[\"clst_final\"]!=df_fusioncand['clst_1st']),'support_read']=df_fusioncand['support_read_1st']\n",
    "    return df_fusioncand\n",
    "\n",
    "\n",
    "def drop_different_genepair_reseq(g1,g2,g1_r,g2_r):\n",
    "    if (g1 in g1_r or g1 in g2_r) and (g2 in g1_r or g2 in g2_r):\n",
    "        res=0\n",
    "    elif (g1_r in g1 or g1_r in g2) and (g2_r in g1 or g2_r in g2):\n",
    "        res=0\n",
    "    else:\n",
    "        res=1\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def filter_out_Nth_hit(df,N):\n",
    "    cond = df.groupby(\"Qname\").Nth_hit.max() == N\n",
    "    filterd_qid = cond[cond].index\n",
    "    df_out=df.query('Qname not in @filterd_qid')\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def filter_refseq_paf(local_file_path_refseq,gtf_path,N=1):\n",
    "    gtf = pd.read_csv(gtf_path, usecols=[0,5],names=['Tname','gene'],header=0,index_col=False)\n",
    "    paf_col = [\"Qname\", \"Qlen\", \"Qstart\", \"Qend\", \"dir\", \"Tname\",\"Tlen\", \"Tstart\", \"Tend\", \"match\", \"block\", \"mapQ\"]\n",
    "    mmap2_refseq = pd.read_table(local_file_path_refseq, index_col=False, header=None,names=paf_col, usecols=range(0, 12)).drop('Tlen', axis=1)\n",
    "    mmap2_refseq['Tname'] = mmap2_refseq['Tname'].str.split(\"_\").str[2]\n",
    "    mmap2_refseq = pd.merge(mmap2_refseq, gtf,how='left')\n",
    "    \n",
    "    #1geneにしか当たらないリードをdrop\n",
    "    cond1=mmap2_refseq.groupby('Qname')['gene'].nunique()!=N\n",
    "    filterd_qid1=cond1[cond1].index\n",
    "    mmap2_refseq_cand1=mmap2_refseq.query('Qname in @filterd_qid1')\n",
    "    \n",
    "    #startとendが一致しているものは落とす\n",
    "    cond2 = (mmap2_refseq_cand1.groupby('Qname')['Qstart'].nunique() != 1) & (mmap2_refseq_cand1.groupby('Qname')['Qend'].nunique() != 1)\n",
    "    filterd_qid2=cond2[cond2].index\n",
    "    mmap2_refseq_cand2=mmap2_refseq_cand1.query('Qname in @filterd_qid2')\n",
    "    mmap2_refseq_cand2['match_rate'] = (mmap2_refseq_cand2['match']) / (mmap2_refseq_cand2['Qend']-mmap2_refseq_cand2['Qstart']+1)\n",
    "    mmap2_refseq_cand2['mapping_rate'] = (mmap2_refseq_cand2['Qend']-mmap2_refseq_cand2['Qstart']+1)/mmap2_refseq_cand2['Qlen']\n",
    "    cond2 = (mmap2_refseq_cand2.groupby('Qname')['Qstart'].nunique() != 1) & (mmap2_refseq_cand2.groupby('Qname')['Qend'].nunique() != 1)\n",
    "    filterd_qid2=cond2[cond2].index\n",
    "    mmap2_refseq_cand2=mmap2_refseq_cand2.query('Qname in @filterd_qid2').sort_values('Qname')\n",
    "    multi_gene_id=mmap2_refseq_cand2.Qname.unique()\n",
    "    \n",
    "    return multi_gene_id,mmap2_refseq, mmap2_refseq_cand2\n",
    "\n",
    "def prep_paf_file_v3(paffile_path, filterd_qid, Nth_hit_flg=1):\n",
    "    paf_col = [\"Qname\", \"Qlen\", \"Qstart\", \"Qend\", \"dir\", \"Tname\",\"Tlen\", \"Tstart\", \"Tend\", \"match\", \"block\", \"mapQ\"]\n",
    "    mmap2 = pd.read_table(paffile_path, index_col=False, header=None,names=paf_col, usecols=range(0, 12)).drop('Tlen', axis=1)\n",
    "    if len(filterd_qid)!=0: \n",
    "        mmap2_cand1=mmap2.query('Qname in @filterd_qid')\n",
    "    else:\n",
    "        mmap2_cand1=mmap2.copy()\n",
    "    cond3 = (mmap2_cand1.groupby('Qname')['Qstart'].nunique() != 1) & (mmap2_cand1.groupby('Qname')['Qend'].nunique() != 1)\n",
    "    filterd_qid3 = cond3[cond3].index\n",
    "    mmap2_cand2 = mmap2_cand1.query('Qname in @filterd_qid3')\n",
    "    mmap2_cand2['Qhit']=mmap2_cand2['Qend']-mmap2_cand2['Qstart']+1\n",
    "    mmap2_cand2['match_rate'] = (mmap2_cand2['match']) / (mmap2_cand2['Qend']-mmap2_cand2['Qstart']+1)\n",
    "    mmap2_cand2['mapping_rate'] = (mmap2_cand2['Qend']-mmap2_cand2['Qstart']+1)/mmap2_cand2['Qlen']\n",
    "    if Nth_hit_flg==1:\n",
    "        mmap2['Tname'] = mmap2['Tname'].str.split(\"_\").str[2]\n",
    "        mmap2_cand2 = add_Nth_hit(mmap2_cand2)\n",
    "    return mmap2,mmap2_cand2\n",
    "\n",
    "def flging_gap_use_read(df):\n",
    "    Qnames=df.query('gene==gene_gap').Qname.unique()\n",
    "    df.loc[df['Qname'].isin(Qnames),'gap_use']=1\n",
    "    return df\n",
    "        \n",
    "def filterout_multigene_hit(df,col):\n",
    "    cond=df.groupby('Qname')[col].nunique()==1\n",
    "    uniq_hit_rid=cond[cond].index\n",
    "    df_filtered=df.query('Qname in @uniq_hit_rid')\n",
    "    return df_filtered\n",
    "\n",
    "def cul_fusioncand_v4(mmap2_paired):\n",
    "    ##print('---------------融合点の計算----------------')\n",
    "    col = [\"Qname\",\"Qstart\",\"Qend\",\"dir\", \"Tname\", \"Tstart\",\"Tend\",\"mapQ\",\"gene\"]\n",
    "    #out_col = [\"hit_rid\", \"Qstart1\",\"Qend1\",\"dir1\", \"chr1\", \"start1\", \"end1\", \"mapQ1\", \"match_rate1\", \"mapping_rate1\",\"Qstart2\",\"Qend2\",\"dir2\", \"chr2\",\n",
    "    #          \"start2\", \"end2\", \"mapQ2\", \"match_rate2\", 'mapping_rate2', \"gap_len\",'gap_rate']\n",
    "    out_col = [\"hit_rid\", \"Qstart1\",\"Qend1\",\"dir1\", \"chr1\", \"start1\", \"end1\", \"mapQ1\",\"gene1\",\"Qstart2\",\"Qend2\",\"dir2\", \"chr2\",\n",
    "               \"start2\", \"end2\", \"mapQ2\", \"gene2\",\"gap_len\",'gap_rate']\n",
    "    arr=[]\n",
    "    for rid in tqdm(mmap2_paired.Qname.unique()):\n",
    "        mmap2_paired_sub = mmap2_paired.query('Qname==@rid')\n",
    "        if mmap2_paired_sub.Nth_hit.nunique()==2:\n",
    "            mmap2_paired_sub_Nth1= mmap2_paired_sub.query('Nth_hit==1')\n",
    "            mmap2_paired_sub_Nth2= mmap2_paired_sub.query('Nth_hit==2')\n",
    "            for i in range(len(mmap2_paired_sub_Nth1)):\n",
    "                for j in range(len(mmap2_paired_sub_Nth2)):\n",
    "                    flont_fp = list(mmap2_paired_sub_Nth1.iloc[i][col]) \n",
    "                    rear_fp = list(mmap2_paired_sub_Nth2.iloc[j][col])[1:]\n",
    "                    gap_len =  mmap2_paired_sub_Nth2.iloc[j]['Qstart']- mmap2_paired_sub_Nth1.iloc[i]['Qend']\n",
    "                    gap_rate =  gap_len / mmap2_paired_sub_Nth1.iloc[i]['Qlen']\n",
    "                    res = flont_fp + rear_fp + [gap_len] + [gap_rate] \n",
    "                    arr.append(res)\n",
    "                    \n",
    "        else:\n",
    "            pass\n",
    "            ### 3箇所以上にマッピングされるものは候補から除外\n",
    "        df_fusioncand=pd.DataFrame(arr)\n",
    "        df_fusioncand.columns=out_col\n",
    "        df_fusioncand = df_fusioncand.drop_duplicates()\n",
    "    return df_fusioncand\n",
    "\n",
    "def change_order_g1g2_v3(df_fusioncand_in,type):\n",
    "    df_fusioncand = df_fusioncand_in.copy()\n",
    "    if type==\"refseq\":\n",
    "        g1_col = ['dir1', 'chr1', 'start1', 'end1','mapQ1','g1']\n",
    "        g2_col = ['dir2', 'chr2', 'start2', 'end2','mapQ2','g2']\n",
    "        condition=(df_fusioncand['g1'] > df_fusioncand['g2']) | ((df_fusioncand['g1'] == df_fusioncand['g2']) & (df_fusioncand['start1'] > df_fusioncand['start2']))\n",
    "    else:\n",
    "        g1_col = ['dir1', 'chr1', 'start1', 'end1','mapQ1' ]\n",
    "        g2_col = ['dir2', 'chr2', 'start2', 'end2','mapQ2']\n",
    "        condition=(df_fusioncand['chr1'] > df_fusioncand['chr2']) | ((df_fusioncand['chr1'] == df_fusioncand['chr2']) & (df_fusioncand['start1'] > df_fusioncand['start2']))\n",
    "    df_cand = df_fusioncand[condition]\n",
    "    new2_df = df_cand[g1_col]\n",
    "    new1_df = df_cand[g2_col]\n",
    "    new1_df.columns=g1_col\n",
    "    new2_df.columns=g2_col\n",
    "    df_fusioncand.loc[condition, g1_col +g2_col] = pd.concat([new1_df, new2_df], axis=1)\n",
    "\n",
    "    return df_fusioncand\n",
    "\n",
    "def get_major_clst(clst1,count1,clst2,count2):\n",
    "    if count1 >= count2:\n",
    "        return clst1\n",
    "    else:\n",
    "        return clst2\n",
    "\n",
    "def make_descendants_table(df,df_count):\n",
    "    import networkx as nx\n",
    "    x=df\n",
    "    G = nx.DiGraph()\n",
    "    G.add_weighted_edges_from([tuple(x) for x in x.values])\n",
    "    nx.info(G)\n",
    "    df_descendants=pd.DataFrame(columns=['clst_1st','clst_final'])\n",
    "    for node in G.nodes():\n",
    "        descendant=nx.descendants(G, node)\n",
    "        if len(descendant)==0:\n",
    "            descendant_=node\n",
    "        else:\n",
    "            descendant_=(list(descendant)[0])\n",
    "        var_ser=pd.Series([node,descendant_],index=df_descendants.columns)\n",
    "        df_descendants=df_descendants.append( var_ser, ignore_index=True )\n",
    "    df_descendants=df_descendants.query('clst_1st!=clst_final')\n",
    "    df_descendants=pd.merge(df_descendants,df_count.drop_duplicates(),on='clst_1st',how='left')\n",
    "    return df_descendants\n",
    "\n",
    "def re_clst_miner2major_v3(df_in,clst_col,clst_count_col,g1_r,g2_r,dont_reclust_cutoff):\n",
    "    df=df_in.copy()\n",
    "    df_clst_count=df[[clst_col,clst_count_col,g1_r,g2_r]].drop_duplicates()\n",
    "    df_4reclst = df[[clst_col,clst_count_col,g1_r,g2_r]]\n",
    "    df_case1=pd.merge(df_4reclst,df_clst_count,on=g1_r,suffixes=['', '_upd1']).query('clst_count<clst_count_upd1')[[clst_col,'clst_1st_upd1',clst_count_col,'clst_count_upd1']].drop_duplicates()\n",
    "    df_case1.columns=df_case1.columns.str.replace('upd1','upd')\n",
    "    df_case2=pd.merge(df_4reclst,df_clst_count,on=g2_r,suffixes=['', '_upd2']).query('clst_count<clst_count_upd2')[[clst_col,'clst_1st_upd2',clst_count_col,'clst_count_upd2']].drop_duplicates()\n",
    "    df_case2.columns=df_case2.columns.str.replace('upd2','upd')\n",
    "\n",
    "    df_clst_f2=pd.concat([df_case1,df_case2]).sort_values('clst_count_upd',ascending=False)\\\n",
    "        .drop_duplicates(clst_col)\\\n",
    "        .query('clst_count<=@dont_reclust_cutoff')\\\n",
    "        .drop(\"clst_count_upd\",axis=1).rename(columns={'clst_1st_upd':'clst_final'})\n",
    "        \n",
    "    \n",
    "    df_clst_count_in=df_clst_count[[clst_col,clst_count_col]].drop_duplicates()\n",
    "    df_clst_f2_upd = make_descendants_table(df_clst_f2,df_clst_count_in) ##ネットワーク使ってアップデート\n",
    "    df_clst_f2_upd2 = make_descendants_table(df_clst_f2_upd,df_clst_count_in)\n",
    "    df_clst_f2_upd3 = make_descendants_table(df_clst_f2_upd2,df_clst_count_in)\n",
    "    assert df_clst_f2_upd3.equals(df_clst_f2_upd2)\n",
    "     \n",
    "    df_out = pd.merge(df,df_clst_f2_upd3,on=[clst_col,clst_count_col],how='left')\n",
    "    df_out.loc[df_out.clst_final.isna(),'clst_final']=df_out[clst_col]\n",
    "    return df_out\n",
    "\n",
    "def get_cytoband_start_end(cytoband,chr,s,e,buffer=15):\n",
    "    cytoband['len']=cytoband['end']-cytoband['start']\n",
    "    cytoband_col=\"chr_pq\"\n",
    "    multi_hit_flg=0\n",
    "    gtf_sub1=cytoband.query('chr==@chr & start-@buffer <= @s <= end + @buffer and chr==@chr & start-@buffer <= @e <= end+@buffer') #gtfの中にstart,endともに張り付いている\n",
    "    if len(gtf_sub1) >= 1:\n",
    "        gtf_sub1= gtf_sub1.sort_values('len',ascending=False)\n",
    "        g_tmp = gtf_sub1[cytoband_col].unique()\n",
    "        g_multi=\"||\".join(g_tmp)\n",
    "        #g=g_tmp[0]\n",
    "        if len(g_tmp) > 1:\n",
    "            multi_hit_flg=1\n",
    "            g_multi=\"||\".join(g_tmp)\n",
    "    else:\n",
    "        cytoband2=cytoband.query('chr==@chr & start-@buffer <= @s <= end+@buffer') #gtfの中にstartのみ張り付いている\n",
    "        cytoband3=cytoband.query('chr==@chr & start-@buffer <= @e <= end+@buffer') #gtfの中にendのみ張り付いている\n",
    "        if len(cytoband2)>=1: \n",
    "            cytoband2= cytoband2.sort_values('len',ascending=False)\n",
    "            g_tmp = cytoband2[cytoband_col].unique()\n",
    "            g_multi=\"||\".join(g_tmp)\n",
    "            #g=g_tmp[0]\n",
    "            if len(g_tmp) > 1:\n",
    "                multi_hit_flg=1\n",
    "        elif len(cytoband3)>=1: \n",
    "            cytoband3= cytoband3.sort_values('len',ascending=False)\n",
    "            g_tmp = cytoband3[cytoband_col].unique()\n",
    "            g_multi=\"||\".join(g_tmp)\n",
    "            #g=g_tmp[0]\n",
    "            if len(g_tmp)>1:\n",
    "                multi_hit_flg=1\n",
    "        \n",
    "    return pd.Series(g_multi)\n",
    "\n",
    "def add_TF_columns_genename_v2(true_gA,true_gB, g1, g2):\n",
    "    res=0;index=9999\n",
    "    # if pd.isna(g1) or pd.isna(g2):\n",
    "    #     return pd.Series([res,index])\n",
    "    for row_g1,true_gA_sub in enumerate(true_gA):\n",
    "        if true_gA_sub in g1 or true_gA_sub in g2:\n",
    "            for row_g2,true_gB_sub in enumerate(true_gB):\n",
    "                if (true_gB_sub in g1 or true_gB_sub in g2) and (row_g1==row_g2):\n",
    "                    res=1\n",
    "                    index=row_g1\n",
    "    return pd.Series([res,index])\n",
    "\n",
    "\n",
    "def make_breakpoint_divX(X,by=10000):\n",
    "    bp=X\n",
    "    chr1=bp.split(\"___\")[0].split(\"__\")[0]\n",
    "    pos1=bp.split(\"___\")[0].split(\"__\")[1]\n",
    "    chr2=bp.split(\"___\")[1].split(\"__\")[0]\n",
    "    pos2=bp.split(\"___\")[1].split(\"__\")[1]\n",
    "    pos1_div=str(my_round_int(int(pos1) / by))\n",
    "    pos2_div=str(my_round_int(int(pos2) / by))\n",
    "    res=chr1+ \"__\" + pos1_div + \"___\" +chr2+ \"__\" + pos2_div\n",
    "    return res\n",
    "\n",
    "def prep_blat_edge_4gaponly(path,topscore=1):\n",
    "    df=pd.read_table(path,names=[\"Qname\",\"Tname\",\"identity\",\"alignment_length\",\"mismatches\",\"gap_openings\",\"Qstart\",\"Qend\",\"Tstart\",\"Tend\",\"evalue\",\"bitscore\"])\n",
    "    if topscore==1:\n",
    "        df=df.sort_values(['Qname','evalue','bitscore'],ascending=[True,True,False])\n",
    "        df=(df.groupby(['Qname'],as_index=False).apply(select,col='evalue',kind='min'))\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "    df.sort_values(['Qname','Qstart'],ascending=[True,True],inplace=True)\n",
    "    #df=df.query('evalue<=@cutoff')\n",
    "    return df\n",
    "\n",
    "def select(df, **kwargs):  # colに列名,kindに最大もしくは最小\n",
    "\n",
    "    if kwargs['kind'] == 'min':\n",
    "        val_r = df[df[kwargs['col']] == min(df[kwargs['col']])]\n",
    "    elif kwargs['kind'] == 'max':\n",
    "        val_r = df[df[kwargs['col']] == max(df[kwargs['col']])]\n",
    "    else:\n",
    "        raise Exception(\"パラメータ不正\")\n",
    "\n",
    "    # 全く同じ行があった場合は削除\n",
    "    val_r = val_r.drop_duplicates()\n",
    "\n",
    "    return val_r\n",
    "\n",
    "def add_exon_s_e(df,gtf_exon_path):\n",
    "    df=df.assign(gene2=df['gene'].str.split(\"\\|\\|\")).explode('gene2')\n",
    "    df_exon=pd.read_csv(gtf_exon_path)\n",
    "    df_merged=pd.merge(df,df_exon[['gene','exstart','exend']],left_on='gene2',right_on=\"gene\")\n",
    "    df_merged['diff_s']=abs(df_merged['Tstart']-df_merged['exstart'])\n",
    "    df_merged['diff_e']=abs(df_merged['exend']-df_merged['Tend'])\n",
    "\n",
    "    tmp1=df_merged.groupby(['Qname','Nth_hit'],as_index=False).diff_s.min()\n",
    "    tmp2=df_merged.groupby(['Qname','Nth_hit'],as_index=False).diff_e.min()\n",
    "    df_merged_t=pd.merge(df_merged,tmp1,how='inner')\n",
    "    df_start=df_merged_t[['Qname','Nth_hit','exstart','diff_s']].drop_duplicates()\n",
    "    df_merged_t=pd.merge(df_merged,tmp2,how='inner')\n",
    "    df_end=df_merged_t[['Qname','Nth_hit','exend','diff_e']].drop_duplicates()\n",
    "    df_s_e=pd.merge(df_start,df_end)\n",
    "    df_out=pd.merge(df,df_s_e).sort_values(['Qname','Nth_hit'])\n",
    "    df_out['diff_s']=df_out['Tstart']-df_out['exstart']\n",
    "    df_out['diff_e']=df_out['exend']-df_out['Tend']\n",
    "    return df_out\n",
    "\n",
    "def fix_start_end(df_in):\n",
    "    df=df_in.copy()\n",
    "    df[['Qstart_fix','Qend_fix','Tstart_fix','Tend_fix']]=df[['Qstart','Qend','Tstart','Tend']].copy()\n",
    "    #Nth1 dir +\n",
    "    df.loc[(df['Nth_hit']==1) & (df['dir']==\"+\"),\"Tend_fix\"]=df['exend']\n",
    "    df.loc[(df['Nth_hit']==1) & (df['dir']==\"+\"),'Qend_fix']=df['Qend']+df['diff_e']\n",
    "    #Nth1 dir -\n",
    "    df.loc[(df['Nth_hit']==1) & (df['dir']==\"-\"),\"Tstart_fix\"]=df['exstart']\n",
    "    df.loc[(df['Nth_hit']==1) & (df['dir']==\"-\"),'Qend_fix']=df['Qend']+df['diff_s']\n",
    "    #df.loc[(df['Nth_hit']==1) & (df['dir']==\"-\")]\n",
    "\n",
    "    #Nth2 dir +\n",
    "    df.loc[(df['Nth_hit']==2) & (df['dir']==\"+\"),\"Tstart_fix\"]=df['exstart']\n",
    "    df.loc[(df['Nth_hit']==2) & (df['dir']==\"+\"),'Qstart_fix']=df['Qstart']-df['diff_s']\n",
    "    #Nth1 dir -\n",
    "    df.loc[(df['Nth_hit']==2) & (df['dir']==\"-\"),\"Tend_fix\"]=df['exend']\n",
    "    df.loc[(df['Nth_hit']==2) & (df['dir']==\"-\"),'Qstart_fix']=df['Qstart']-df['diff_e']\n",
    "    return df\n",
    "            \n",
    "\n",
    "def judge_cross_over_v3(mmap2_paired,buffer,s,e):\n",
    "    ##print('クエリ側のクロスオーバーの判定')\n",
    "    rids=[]\n",
    "    if mmap2_paired['Nth_hit'].value_counts()[1] != mmap2_paired['Nth_hit'].value_counts()[2]:\n",
    "        drop_rid_nopaired = pd.DataFrame(mmap2_paired.groupby('Qname')['Nth_hit'].nunique() == 1).query('Nth_hit==True').index\n",
    "        mmap2_paired = mmap2_paired.query('Qname not in @drop_rid_nopaired')\n",
    "    for x,rid in enumerate(tqdm(mmap2_paired.Qname.unique())): #read_idごと\n",
    "        mmap2_paired_sub = mmap2_paired.query('Qname==@rid')\n",
    "        N=mmap2_paired_sub.Nth_hit.min()\n",
    "        first_end_pos = int(mmap2_paired_sub.query('Nth_hit==@N')[e].max())\n",
    "        try:\n",
    "            second_start_pos = int(mmap2_paired_sub.query('Nth_hit==@N+1')[s].min())\n",
    "        except:\n",
    "            assert mmap2_paired_sub['Nth_hit'].nunique()==2 ,'Nth_hitが3以上です'\n",
    "            second_start_pos = int(mmap2_paired_sub.query('Nth_hit!=@N')[s].min())\n",
    "        if first_end_pos - buffer > second_start_pos:\n",
    "            rids.append(rid)\n",
    "    return rids\n",
    "\n",
    "def add_exon_s_e(df,gtf_exon_path):\n",
    "    df=df.assign(gene2=df['gene'].str.split(\"\\|\\|\")).explode('gene2')\n",
    "    df_exon=pd.read_csv(gtf_exon_path)\n",
    "    df_merged=pd.merge(df,df_exon[['gene','exstart','exend']],left_on='gene2',right_on=\"gene\")\n",
    "    df_merged['diff_s']=abs(df_merged['Tstart']-df_merged['exstart'])\n",
    "    df_merged['diff_e']=abs(df_merged['exend']-df_merged['Tend'])\n",
    "\n",
    "    tmp1=df_merged.groupby(['Qname','Nth_hit'],as_index=False).diff_s.min()\n",
    "    tmp2=df_merged.groupby(['Qname','Nth_hit'],as_index=False).diff_e.min()\n",
    "    df_merged_t=pd.merge(df_merged,tmp1,how='inner')\n",
    "    df_start=df_merged_t[['Qname','Nth_hit','exstart','diff_s']].drop_duplicates()\n",
    "    df_merged_t=pd.merge(df_merged,tmp2,how='inner')\n",
    "    df_end=df_merged_t[['Qname','Nth_hit','exend','diff_e']].drop_duplicates()\n",
    "    df_s_e=pd.merge(df_start,df_end)\n",
    "    df_out=pd.merge(df,df_s_e).sort_values(['Qname','Nth_hit'])\n",
    "    df_out['diff_s']=df_out['Tstart']-df_out['exstart']\n",
    "    df_out['diff_e']=df_out['exend']-df_out['Tend']\n",
    "    return df_out\n",
    "\n",
    "def fix_start_end(df_in):\n",
    "    df=df_in.copy()\n",
    "    df[['Qstart_fix','Qend_fix','Tstart_fix','Tend_fix']]=df[['Qstart','Qend','Tstart','Tend']].copy()\n",
    "    #Nth1 dir +\n",
    "    df.loc[(df['Nth_hit']==1) & (df['dir']==\"+\"),\"Tend_fix\"]=df['exend']\n",
    "    df.loc[(df['Nth_hit']==1) & (df['dir']==\"+\"),'Qend_fix']=df['Qend']+df['diff_e']\n",
    "    #Nth1 dir -\n",
    "    df.loc[(df['Nth_hit']==1) & (df['dir']==\"-\"),\"Tstart_fix\"]=df['exstart']\n",
    "    df.loc[(df['Nth_hit']==1) & (df['dir']==\"-\"),'Qend_fix']=df['Qend']+df['diff_s']\n",
    "    #df.loc[(df['Nth_hit']==1) & (df['dir']==\"-\")]\n",
    "\n",
    "    #Nth2 dir +\n",
    "    df.loc[(df['Nth_hit']==2) & (df['dir']==\"+\"),\"Tstart_fix\"]=df['exstart']\n",
    "    df.loc[(df['Nth_hit']==2) & (df['dir']==\"+\"),'Qstart_fix']=df['Qstart']-df['diff_s']\n",
    "    #Nth1 dir -\n",
    "    df.loc[(df['Nth_hit']==2) & (df['dir']==\"-\"),\"Tend_fix\"]=df['exend']\n",
    "    df.loc[(df['Nth_hit']==2) & (df['dir']==\"-\"),'Qstart_fix']=df['Qstart']-df['diff_e']\n",
    "    return df\n",
    "            \n",
    "def prep_subseq_read_gaponly_v2(df_in):\n",
    "    res_df=pd.DataFrame()\n",
    "    ##print('----start prep_subseq_read gap only -------')\n",
    "    df = df_in[['Qname','Qlen' ,'Qstart_fix', 'Qend_fix','Nth_hit']].drop_duplicates()\n",
    "    out_col=['Qname', 'Qstart', 'Qend']\n",
    "    for rid in tqdm(df.Qname.unique()):\n",
    "        df_sub = df.query('Qname in @rid')\n",
    "        Nths = df_sub.Nth_hit.nunique()\n",
    "        if Nths==2: \n",
    "            df_sub1=df_sub.query('Nth_hit == 1')\n",
    "            df_sub2=df_sub.query('Nth_hit == 2')\n",
    "            N=0\n",
    "            for (_,r1) in df_sub1.iterrows():\n",
    "                for (_,r2) in df_sub2.iterrows():\n",
    "                    res = pd.Series([rid, int(r1[\"Qend_fix\"]),int(r2[\"Qstart_fix\"])],index=out_col)\n",
    "                    res_df = res_df.append(res,ignore_index=True)\n",
    "                    N=N+1\n",
    "    return res_df\n",
    "\n",
    "def judge_cross_over_v3(mmap2_paired,buffer,s,e):\n",
    "    ##print('クエリ側のクロスオーバーの判定')\n",
    "    rids=[]\n",
    "    if mmap2_paired['Nth_hit'].value_counts()[1] != mmap2_paired['Nth_hit'].value_counts()[2]:\n",
    "        drop_rid_nopaired = pd.DataFrame(mmap2_paired.groupby('Qname')['Nth_hit'].nunique() == 1).query('Nth_hit==True').index\n",
    "        mmap2_paired = mmap2_paired.query('Qname not in @drop_rid_nopaired')\n",
    "    for x,rid in enumerate(tqdm(mmap2_paired.Qname.unique())): #read_idごと\n",
    "        mmap2_paired_sub = mmap2_paired.query('Qname==@rid')\n",
    "        N=mmap2_paired_sub.Nth_hit.min()\n",
    "        first_end_pos = int(mmap2_paired_sub.query('Nth_hit==@N')[e].max())\n",
    "        try:\n",
    "            second_start_pos = int(mmap2_paired_sub.query('Nth_hit==@N+1')[s].min())\n",
    "        except:\n",
    "            assert mmap2_paired_sub['Nth_hit'].nunique()==2 ,'Nth_hitが3以上です'\n",
    "            second_start_pos = int(mmap2_paired_sub.query('Nth_hit!=@N')[s].min())\n",
    "        if first_end_pos - buffer > second_start_pos:\n",
    "            rids.append(rid)\n",
    "    return rids\n",
    "    \n",
    "def split_bp(bp):\n",
    "    chr1=bp.split(\"___\")[0].split(\"__\")[0]\n",
    "    pos1=int(bp.split(\"___\")[0].split(\"__\")[1])\n",
    "    chr2=bp.split(\"___\")[1].split(\"__\")[0]\n",
    "    pos2=int(bp.split(\"___\")[1].split(\"__\")[1])\n",
    "    return pd.Series(list([chr1,pos1,chr2,pos2]))\n",
    "\n",
    "def make_bp_clst_mode(df):\n",
    "    df[[\"name2_chr1\",\"name2_pos1\",\"name2_chr2\",\"name2_pos2\"]]=df['name2'].apply(lambda x:split_bp(x))\n",
    "    df['g1_g2_clst'] = df['g1_clst'] + \"--\" + df[\"g2_clst\"]\n",
    "    \n",
    "    tmp=pd.DataFrame(df.groupby('g1_g2_clst',as_index=True)['name2_pos1'].apply(lambda x: x.mode())).reset_index().drop('level_1',axis=1).rename(columns={'name2_pos1':'name2_pos1_mode'})\n",
    "    tmp2=pd.DataFrame(df.groupby('g1_g2_clst',as_index=True)['name2_pos2'].apply(lambda x: x.mode())).reset_index().drop('level_1',axis=1).rename(columns={\"name2_pos2\":'name2_pos2_mode'})\n",
    "    df_bp_mode=pd.merge(tmp,tmp2)\n",
    "    df=pd.merge(df,df_bp_mode,how='left',on='g1_g2_clst')\n",
    "    df[\"name2_clst\"]=df['name2_chr1']+\"__\"+df['name2_pos1_mode'].astype(str)+\"___\"+df['name2_chr2']+\"__\"+df['name2_pos2_mode'].astype(str)\n",
    "    df=df.drop(['name2_chr1','name2_chr2','name2_pos1','name2_pos2'],axis=1)\n",
    "    return df\n",
    "\n",
    "def cul_gaplen(df_in,Qstart,Qend):\n",
    "    df = df_in[['Qname','Qlen' ,Qstart, Qend,'Nth_hit']].drop_duplicates()\n",
    "    res_df=pd.DataFrame()\n",
    "    for rid in tqdm(df.Qname.unique()):\n",
    "        df_sub = df.query('Qname in @rid')\n",
    "        Nths = df_sub.Nth_hit.nunique()\n",
    "        if Nths==2: \n",
    "            df_sub1=df_sub.query('Nth_hit == 1')\n",
    "            df_sub2=df_sub.query('Nth_hit == 2')\n",
    "            N=0\n",
    "            for (_,r1) in df_sub1.iterrows():\n",
    "                for (_,r2) in df_sub2.iterrows():\n",
    "                    res = pd.Series([rid, int(r2[Qstart])-int(r1[Qend])],index=['rid','gaplen'])\n",
    "                    res_df = res_df.append(res,ignore_index=True)\n",
    "                    N=N+1\n",
    "    return res_df\n",
    "\n",
    "def bp_update_v3(df_in):\n",
    "    df=df_in.copy()\n",
    "    df['Tstart_exon']=df['Tstart']\n",
    "    df['Tend_exon']=df['Tend']\n",
    "    df.loc[(df['gene_gap_equal']==1)&(df[\"Nth_hit\"]==1)&(df[\"dir\"]==\"+\"),\"Tend\"]=df[\"Tend_gap\"]\n",
    "    df.loc[(df['gene_gap_equal']==1)&(df[\"Nth_hit\"]==1)&(df[\"dir\"]==\"-\"),\"Tstart\"]=df[\"Tstart_gap\"]\n",
    "    df.loc[(df['gene_gap_equal']==1)&(df[\"Nth_hit\"]==2)&(df[\"dir\"]==\"-\"),\"Tend\"]=df[\"Tend_gap\"]\n",
    "    df.loc[(df['gene_gap_equal']==1)&(df[\"Nth_hit\"]==2)&(df[\"dir\"]==\"+\"),\"Tstart\"]=df[\"Tstart_gap\"]\n",
    "    return df\n",
    "\n",
    "def flging_gap_use_read_v2(df):\n",
    "    Qnames=df.query('(gene_gap.notna()) and( gene in gene_gap or gene_gap in gene)').Qname.unique()\n",
    "    #print(len(Qnames))\n",
    "    df['gap_use']=0\n",
    "    df.loc[df['Qname'].isin(Qnames),'gap_use']=1\n",
    "    return df\n",
    "def check_gene_genegap(gene,gene_gap):\n",
    "    if pd.isna(gene) or pd.isna(gene_gap):\n",
    "        return 0\n",
    "    elif gene in gene_gap or gene_gap in gene:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def bp_update_v4(df_in):\n",
    "    df=df_in.copy()\n",
    "    #df['Tstart_exon']=df['Tstart']\n",
    "    #df['Tend_exon']=df['Tend']\n",
    "    df['gap_bp_hosei']=0\n",
    "    df.loc[(abs(df['Tstart_gap']-df['exstart'])<=buffer) | (abs(df['Tstart_gap']-df['exend'])<=buffer),\"gap_bp_hosei\"]=1\n",
    "    df.loc[(abs(df['Tend_gap']-df['exstart'])<=buffer) | (abs(df['Tend_gap']-df['exend'])<=buffer),\"gap_bp_hosei\"]=1\n",
    "    df.loc[(df['gap_bp_hosei']==1)&(df['gene_gap_equal']==1)&(df[\"Nth_hit\"]==1)&(df[\"dir\"]==\"+\"),\"Tend\"]=df[\"exend\"]\n",
    "    df.loc[(df['gap_bp_hosei']==1)&(df['gene_gap_equal']==1)&(df[\"Nth_hit\"]==1)&(df[\"dir\"]==\"-\"),\"Tstart\"]=df[\"exstart\"]\n",
    "    df.loc[(df['gap_bp_hosei']==1)&(df['gene_gap_equal']==1)&(df[\"Nth_hit\"]==2)&(df[\"dir\"]==\"-\"),\"Tend\"]=df[\"exend\"]\n",
    "    df.loc[(df['gap_bp_hosei']==1)&(df['gene_gap_equal']==1)&(df[\"Nth_hit\"]==2)&(df[\"dir\"]==\"+\"),\"Tstart\"]=df[\"exstart\"]\n",
    "    return df\n",
    "\n",
    "def make_name_col_nodev(df):\n",
    "    tmp = df.query('dir1==\"+\" & dir2 == \"+\"')\n",
    "    tmp[\"name2\"] = tmp[\"chr1\"] + \"__\" + tmp[\"end1\"].astype(str) + \"___\" +  tmp[\"chr2\"] + \"__\" + tmp[\"start2\"].astype(str)\n",
    "    tmp2 = df.query('dir1==\"+\"& dir2 == \"-\"')\n",
    "    tmp2[\"name2\"]= tmp2[\"chr1\"] + \"__\" + tmp2[\"end1\"].astype(str) + \"___\" + tmp2[\"chr2\"] + \"__\" + tmp2[\"end2\"].astype(str)\n",
    "    tmp3 = df.query('dir1==\"-\" & dir2 == \"+\"')\n",
    "    tmp3[\"name2\"] = tmp3[\"chr1\"] + \"__\" + tmp3[\"start1\"].astype(str) + \"___\" + tmp3[\"chr2\"] + \"__\" + tmp3[\"start2\"].astype(str)\n",
    "    tmp4 = df.query('dir1==\"-\" & dir2 == \"-\"')\n",
    "    tmp4[\"name2\"]= tmp4[\"chr1\"] + \"__\" + tmp4[\"start1\"].astype(str) + \"___\" + tmp4[\"chr2\"] + \"__\" + tmp4[\"end2\"].astype(str)\n",
    "    df=pd.concat([tmp,tmp2], ignore_index=True, axis=0)\n",
    "    df=pd.concat([df,tmp3], ignore_index=True, axis=0)\n",
    "    df=pd.concat([df,tmp4], ignore_index=True, axis=0)\n",
    "    return df\n",
    "\n",
    "def add_g1_g2_clst_col_v3(df_fusioncand_clst_tmp,clst_col,g1_col,g2_col,out_g1_col,out_g2_col,out_g1_pct_col,out_g2_pct_col,criteria):\n",
    "   res_df=pd.DataFrame()\n",
    "   for clst in df_fusioncand_clst_tmp[clst_col].unique():\n",
    "      df_fusioncand_clust_sub= df_fusioncand_clst_tmp[df_fusioncand_clst_tmp[clst_col]==clst]\n",
    "      ratio1 = df_fusioncand_clust_sub[g1_col].value_counts(normalize=True)\n",
    "      ratio2 = df_fusioncand_clust_sub[g2_col].value_counts(normalize=True)\n",
    "      cond1 = ratio1 >= criteria\n",
    "      cond2 = ratio2 >= criteria\n",
    "      g1_multi_clst=\"_\".join(list((cond1.index[cond1])))\n",
    "      g2_multi_clst=\"_\".join(list((cond2.index[cond2])))\n",
    "      g1_multi_clst_pct=\"_\".join(list((ratio1[cond1].round(2)*100).astype(int).astype(str) + '%'))\n",
    "      g2_multi_clst_pct=\"_\".join(list((ratio2[cond2].round(2)*100).astype(int).astype(str) + '%'))\n",
    "      #g1_multi_clst_pct=ratio1[cond1][0].round(2)\n",
    "      #g2_multi_clst_pct=ratio2[cond2][0].round(2)\n",
    "      \n",
    "      df_fusioncand_clust_sub[out_g1_col]=g1_multi_clst\n",
    "      df_fusioncand_clust_sub[out_g2_col]=g2_multi_clst\n",
    "      df_fusioncand_clust_sub[out_g1_pct_col]=g1_multi_clst_pct\n",
    "      df_fusioncand_clust_sub[out_g2_pct_col]=g2_multi_clst_pct\n",
    "      #res_df = res_df.append(df_fusioncand_clust_sub)\n",
    "      res_df=pd.concat([res_df,df_fusioncand_clust_sub], ignore_index=True, axis=0)\n",
    "\n",
    "   return res_df\n",
    "def add_clst_count_v2(df,clst_col,support_flg=1):\n",
    "    df_clst_count = df[clst_col].value_counts().to_frame().reset_index().rename(columns={\"count\": \"clst_count\"})\n",
    "    df_out=pd.merge(df,df_clst_count,on=clst_col)\n",
    "    if support_flg==1:\n",
    "        df_out[\"support_read\"]=df_out.apply(lambda x:int(x[\"clst_count\"])*float(x['g1_clst_pct'].split(\"_\")[0].replace(\"%\",\"\"))*float(x['g2_clst_pct'].split(\"_\")[0].replace(\"%\",\"\"))*0.01*0.01,axis=1)\n",
    "    else:\n",
    "        pass\n",
    "    assert df[clst_col].nunique()==df_out[clst_col].nunique()\n",
    "    return df_out\n",
    "\n",
    "def make_bp_clst_mode_v2(df):\n",
    "    df[[\"name2_chr1\",\"name2_pos1\",\"name2_chr2\",\"name2_pos2\"]]=df['name2'].apply(lambda x:split_bp(x))\n",
    "    tmp1=df.groupby('g1_g2_clst',as_index=True)['name2_pos1'].value_counts().to_frame().rename(columns={\"count\":\"name2_pos1_count\"}).reset_index().rename(columns={'name2_pos1':'name2_pos1_mode'})#.drop_duplicates(\"g1_g2_clst\")\n",
    "    tmp2=df.groupby('g1_g2_clst',as_index=True)['name2_pos2'].value_counts().to_frame().rename(columns={\"count\":\"name2_pos2_count\"}).reset_index().rename(columns={'name2_pos2':'name2_pos2_mode'})#.drop_duplicates(\"g1_g2_clst\")\n",
    "    tmp3=pd.merge(tmp1.groupby('g1_g2_clst')[\"name2_pos1_mode\"].apply(list).to_frame().reset_index(),tmp1.groupby('g1_g2_clst')[\"name2_pos1_count\"].apply(list).to_frame().reset_index()).rename(columns={'name2_pos1_mode':'name2_pos1_mode_list','name2_pos1_count':'name2_pos1_count_list'})\n",
    "    tmp4=pd.merge(tmp2.groupby('g1_g2_clst')[\"name2_pos2_mode\"].apply(list).to_frame().reset_index(),tmp2.groupby('g1_g2_clst')[\"name2_pos2_count\"].apply(list).to_frame().reset_index()).rename(columns={'name2_pos2_mode':'name2_pos2_mode_list','name2_pos2_count':'name2_pos2_count_list'})\n",
    "    df_bp_mode=pd.merge(tmp1.drop_duplicates(\"g1_g2_clst\"),pd.merge(tmp2.drop_duplicates(\"g1_g2_clst\"),pd.merge(tmp3,tmp4)))\n",
    "    df=pd.merge(df,df_bp_mode,how='left',on='g1_g2_clst')\n",
    "    df[\"name2_clst\"]=df['name2_chr1']+\"__\"+df['name2_pos1_mode'].astype(str)+\"___\"+df['name2_chr2']+\"__\"+df['name2_pos2_mode'].astype(str)\n",
    "    df=df.drop(['name2_chr1','name2_chr2','name2_pos1','name2_pos2'],axis=1)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edgeアライメント後 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:00<00:00, 123.98it/s]\n",
      "20it [00:00, 685.99it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 230.03it/s]\n"
     ]
    }
   ],
   "source": [
    "root = Path.cwd().parent\n",
    "\n",
    "#input\n",
    "paf_dir = root / f\"out/\" \n",
    "paffile_name = f\"{TARGET}_hg19.paf\"\n",
    "paffile_name_refseq=f\"{TARGET}_refseq.paf\"\n",
    "gap_genome_blat_name = f\"{TARGET}_gap_blat_min15_stp5.psl\"\n",
    "paffile_path = paf_dir /paffile_name\n",
    "paffile_refseq_path = paf_dir/paffile_name_refseq\n",
    "gap_genome_blat_path = paf_dir / gap_genome_blat_name\n",
    "\n",
    "#outdir\n",
    "res_file_dir = root / f\"out/\" \n",
    "intermediate_file_dir = res_file_dir / f\"intermediate\"\n",
    "res_genome_dir = res_file_dir / f\"genome\"\n",
    "res_genome_path = res_genome_dir / f\"{TARGET}_df_fusioncand_genome.csv\"\n",
    "res_refseq_path = res_genome_dir / f\"{TARGET}_df_fusioncand_refseq.csv\"\n",
    "res_4_edge_alignment_dir = res_file_dir / \"for_edge_alignment/\"\n",
    "for_make_edge_file_path = res_4_edge_alignment_dir / f\"{TARGET}/gap_4make_edge.csv\"\n",
    "output_file_path = res_file_dir / f'df_res_{TARGET}.csv'\n",
    "\n",
    "#gtf\n",
    "gtf_path = root / \"data/ref/hg19_genCode19.tab.usecol\" #refseqだけにしない 遺伝子のstart endのみ。エキソンごとではない\n",
    "gtf_exon_path = root / \"data/ref/hg19_genCode19.tab.exon\" #refseqだけにしない 遺伝子のstart endのみ。エキソンごとではない\n",
    "\n",
    "mmap2_align2genome_path =  intermediate_file_dir / f\"{TARGET}_genome.csv\"\n",
    "mmap2_align2refseq_path =  intermediate_file_dir / f\"{TARGET}_refseq.csv\"\n",
    "out_col=['TARGET','hit_rid','g1_clst','g2_clst','g1_g2_clst','name','name2_clst','support_read']\n",
    "\n",
    "#### ファイル読み込み--------------------------------------------------------------------------------------------\n",
    "gtf = prep_gtf(pd.read_csv(gtf_path))\n",
    "multihit_id,mmap2_refseq_org, mmap2_refseq = filter_refseq_paf(paffile_refseq_path, gtf_path)  \n",
    "edge_start_end_ov = pd.read_csv(for_make_edge_file_path).rename(columns={'length':'gap_len_tmp'})\n",
    "mmap2_Nth2_rmcross_rmdir = pd.read_csv(mmap2_align2genome_path)\n",
    "mmap2_Nth2_rmcross_rmdir=mmap2_Nth2_rmcross_rmdir.rename(columns={'Qstart':'Qstart_org',\"Qend\":\"Qend_org\",\"Tstart\":\"Tstart_org\",\"Tend\":\"Tend_org\",\"Qstart_fix\":\"Qstart\",\"Qend_fix\":\"Qend\",\"Tstart_fix\":\"Tstart\",\"Tend_fix\":\"Tend\"})\n",
    "\n",
    "#gapがヒットする遺伝子名を取得\n",
    "mmap2_gap=prep_blat_edge_4gaponly(gap_genome_blat_path)\n",
    "mmap2_gap=pd.merge(mmap2_gap,edge_start_end_ov[['Qname','gap_len_tmp']],on=\"Qname\")\n",
    "mmap2_gap['gap_mapping_rate']=mmap2_gap['alignment_length']/mmap2_gap['gap_len_tmp']\n",
    "mmap2_gap=mmap2_gap.query('gap_mapping_rate>=@gap_mapping_rate_cutoff').query('evalue<=@gap_evalue_cutoff')\n",
    "mmap2_gap['gene_gap']=mmap2_gap.apply(lambda x: get_gene_name_start_end(gtf, x['Tname'],x['Tstart'],x['Tend']), axis=1)\n",
    "mmap2_gap_use=filterout_multigene_hit(mmap2_gap,\"gene_gap\").query('gene_gap!=\"intron\"')\n",
    "\n",
    "### gapの配列を使ってフィルタ----------------------------------------------------------------------------------------------------------\n",
    "### gapの配列の結果とgap長をマージ\n",
    "mmap2_Nth2_rmcross_rmdup_gap =pd.merge(mmap2_Nth2_rmcross_rmdir,mmap2_gap_use[['Qname','Tname','Tend','Tstart','gene_gap','alignment_length']],on=['Qname','Tname'],how='left',suffixes=['','_gap'])\n",
    "mmap2_Nth2_rmcross_rmdup_gap = pd.merge(mmap2_Nth2_rmcross_rmdup_gap,edge_start_end_ov[['Qname','gap_len_tmp']],how='left',on=['Qname'])\n",
    "\n",
    "# genoem とgap でgene名が同じもののみ採用する\n",
    "mmap2_Nth2_rmcross_rmdup_gap=mmap2_Nth2_rmcross_rmdup_gap.query('gene!=\"intron\"') #gap_len_tmpが50より大きく、gapもヒットしないものはdrop\n",
    "mmap2_Nth2_rmcross_rmdup_gap_use=flging_gap_use_read_v2(mmap2_Nth2_rmcross_rmdup_gap)\n",
    "mmap2_Nth2_rmcross_rmdup_gap_use[\"gene_gap_equal\"]=mmap2_Nth2_rmcross_rmdup_gap_use.apply(lambda x:check_gene_genegap(x[\"gene\"],x['gene_gap']),axis=1)\n",
    "\n",
    "# gapでヒットするupdate\n",
    "mmap2_Nth2_rmcross_rmdup_gap_use=bp_update_v3(mmap2_Nth2_rmcross_rmdup_gap_use)  #v24.4.2.2\n",
    "mmap2_Nth2_rmcross_rmdup_gap_use=fix_start_end(mmap2_Nth2_rmcross_rmdup_gap_use)  #v24.4.2\n",
    "\n",
    "#  フラグメントの融合のペアを算出-----------------------------------------------------------------------------------------\n",
    "df_fusioncand_g = cul_fusioncand_v4(mmap2_Nth2_rmcross_rmdup_gap_use)\n",
    "\n",
    "#  genome側のクロスオーバー除去----------------------------------------------------------------------------------------\n",
    "df_fusioncand_g,drop_rid_cross_over_genome = rm_cross_over(df_fusioncand_g)\n",
    "df_fusioncand_g=df_fusioncand_g.astype({\"start1\":int,\"end1\":int,\"start2\":int,\"end2\":int})\n",
    "\n",
    "### gapでフィルタ---------------------------------------------------------------------------------------------------------\n",
    "df_fusioncand_g_gap = pd.merge(df_fusioncand_g,mmap2_gap_use[['Qname','gene_gap']],left_on=\"hit_rid\",right_on=\"Qname\",how='left')\n",
    "df_fusioncand_g_gap_use=df_fusioncand_g_gap.copy()\n",
    "\n",
    "# gapと遺伝子名が違うものは除去\n",
    "df_fusioncand_g_gap_use=pd.merge(df_fusioncand_g_gap_use,mmap2_Nth2_rmcross_rmdup_gap_use.groupby('Qname',as_index=False)['gene_gap_equal'].max(),how=\"left\").drop(\"Qname\",axis=1)\n",
    "df_fusioncand_g_gap_use = df_fusioncand_g_gap_use.query('-1*@buffer<=gap_len<=@gap_len_cutoff or gene_gap_equal==1') #v24.4.2.3\n",
    "\n",
    "# ### breakpointを特定---------------------------------------------------------------------------------------------------------\n",
    "df_fusioncand_g_gap_use= make_name_col_nodev(df_fusioncand_g_gap_use)\n",
    "df_fusioncand_g_gap_use = change_order_g1g2_v3(df_fusioncand_g_gap_use,\"genome\").rename(columns={'gene1':'g1','gene2':'g2'})\n",
    "\n",
    "#name2の順番入れ替え\n",
    "df_fusioncand_g_gap_use['name2']=df_fusioncand_g_gap_use['name2'].apply(lambda x: \"___\".join(sorted([x.split(\"___\")[0],x.split(\"___\")[1]],reverse=False)))\n",
    "\n",
    "### 2geneに当たるもののみ採用\n",
    "df_fusioncand_g_gap_use_2gene  = filter_only_2pairofgene(df_fusioncand_g_gap_use,'g1','g2')\n",
    "\n",
    "# # #intronにヒットするものやg1・g2が包含されるものは除く（除ききれなかったスプライシング）\n",
    "df_fusioncand_g_gap_use_2gene_filtered = df_fusioncand_g_gap_use_2gene.query('g1!=\"intron\"&g2!=\"intron\"')\n",
    "df_fusioncand_g_gap_use_2gene_filtered = df_fusioncand_g_gap_use_2gene_filtered[df_fusioncand_g_gap_use_2gene_filtered.apply(lambda x: (x['g1'] not in x['g2']) and ( x['g2'] not in x['g1']),axis=1)]\n",
    "\n",
    "### transcriptの結果を使うための前処理\n",
    "##intronにヒットするものやg1・g2が包含されるものは除く\n",
    "rid = df_fusioncand_g_gap_use_2gene_filtered.hit_rid.unique()\n",
    "mmap2_refseq_use = mmap2_refseq.query('Qname in @rid')\n",
    "mmap2_refseq_use = mmap2_refseq_use.drop_duplicates(subset=['Qname','gene'])\n",
    "mmap2_refseq_use = add_Nth_hit(mmap2_refseq_use)\n",
    "df_fusioncand_r = cul_fusioncand_v4_4refseq(mmap2_refseq_use)\n",
    "df_refseq_r_tmp = df_fusioncand_r[['hit_rid','g1','g2']].drop_duplicates()\n",
    "df_fusioncand_g_gap_use_2gene_filtered_refseq = pd.merge(df_fusioncand_g_gap_use_2gene_filtered,df_refseq_r_tmp,on='hit_rid',suffixes=['','_r'],how='inner')\n",
    "\n",
    "### genomeのみ検出を捨てる\n",
    "df_fusioncand_g_gap_use_2gene_filtered_refseq.query('g1_r.notna() and g2_r.notna()',inplace=True)\n",
    "df_fusioncand_g_gap_use_2gene_filtered_refseq[['g1_r','g2_r']] = df_fusioncand_g_gap_use_2gene_filtered_refseq.apply(lambda x: sorted(pd.Series([x['g1_r'], x['g2_r']]), reverse=False), axis=1,result_type='expand')\n",
    "\n",
    "# ##refsqも同様g1・g2が包含されるものは除く\n",
    "df_fusioncand_g_gap_use_2gene_filtered_refseq_filtered = df_fusioncand_g_gap_use_2gene_filtered_refseq[df_fusioncand_g_gap_use_2gene_filtered_refseq.apply(lambda x: (x['g1_r'] not in x['g2_r']) and ( x['g2_r'] not in x['g1_r']),axis=1)]\n",
    "df_fusioncand_g_gap_use_2gene_filtered_refseq_filtered['diff_refseq']=df_fusioncand_g_gap_use_2gene_filtered_refseq_filtered.apply(lambda x:drop_different_genepair_reseq(x['g1'],x['g2'],x['g1_r'],x['g2_r']),axis=1)\n",
    "df_fusioncand_g_gap_use_2gene_filtered_refseq_filtered = df_fusioncand_g_gap_use_2gene_filtered_refseq_filtered.query('diff_refseq==0')\n",
    "\n",
    "### クラスタリング準備--------------------------------------------------------------------------------------------------------------------------------\n",
    "df_fusioncand_g_gap_use_2gene_filtered_refseq_filtered[\"name\"]=df_fusioncand_g_gap_use_2gene_filtered_refseq_filtered.apply(lambda x:make_breakpoint_divX(x['name2'],clst_bp),axis=1)\n",
    "\n",
    "df_fusioncand_4clst=df_fusioncand_g_gap_use_2gene_filtered_refseq_filtered.query('mapQ1!=0&mapQ2!=0')\n",
    "df_fusioncand_4clst=df_fusioncand_4clst.query('-1*@buffer<=gap_len<=@gap_len_cutoff or gene_gap_equal==1')\n",
    "df_fusioncand_4clst = add_g1_g2_clst_col_v3(df_fusioncand_4clst, 'name', 'g1_r', 'g2_r', 'g1_clst', 'g2_clst', 'g1_clst_pct', 'g2_clst_pct',clst_percent_cutoff)\n",
    "\n",
    "df_fusioncand_4clst_major=df_fusioncand_4clst.query('g1_clst!=\"\"').query('g2_clst!=\"\"')\n",
    "df_fusioncand_4clst_major=add_clst_count_v2(df_fusioncand_4clst_major, 'name',1).sort_values(['clst_count','gap_len'],ascending=[False,True])\n",
    "df_fusioncand_4clst_major=df_fusioncand_4clst_major.drop_duplicates(\"hit_rid\")\n",
    "df_fusioncand_4clst_major['g1_g2_clst'] = df_fusioncand_4clst_major['g1_clst'] + \"--\" + df_fusioncand_4clst_major[\"g2_clst\"]\n",
    "\n",
    "#bpを最頻値で\n",
    "df_fusioncand_4clst_major_bp=make_bp_clst_mode_v2(df_fusioncand_4clst_major)\n",
    "\n",
    "## データ出力 -------------------------------------------------------------------------------------------------------------------------------\n",
    "df_fusioncand_4clst_major_bp.insert(0, 'TARGET', TARGET)   \n",
    "df_out=df_fusioncand_4clst_major_bp[out_col].drop_duplicates(['hit_rid','g1_g2_clst'])\n",
    "df_out.to_csv(output_file_path, index=False)\n",
    "#df_fusioncand_4clst_major_bp[['TARGET','hit_rid','g1_g2_clst','support_read','name2_clst','gap_len']].to_csv(output_file_path, index=False)\n",
    "#df_fusioncand_4clst_major_bp.query('name2_pos1_count>1 and name2_pos2_count>1').to_csv(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "215bd58fd8531144f9200bfb3bbe6bdfea4498d65455a6613b4bae8dff07dfad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
